"""
AuraWellæ€§èƒ½ç›‘æ§å™¨
åå°è¿è¡Œçš„æ€§èƒ½ç›‘æ§æœåŠ¡ï¼Œå®šæœŸæ”¶é›†å’Œåˆ†æç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
"""

import asyncio
import logging
import time
import psutil
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
from dataclasses import dataclass, asdict
from collections import deque

logger = logging.getLogger(__name__)


@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    disk_usage_percent: float
    response_time_ms: float
    error_rate: float
    active_connections: int


@dataclass
class AlertThreshold:
    """å‘Šè­¦é˜ˆå€¼é…ç½®"""
    cpu_threshold: float = 80.0
    memory_threshold: float = 85.0
    disk_threshold: float = 90.0
    response_time_threshold: float = 5000.0  # 5ç§’
    error_rate_threshold: float = 0.05  # 5%


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨ä¸»ç±»"""
    
    def __init__(self, 
                 collection_interval: int = 60,  # æ”¶é›†é—´éš”ï¼ˆç§’ï¼‰
                 retention_hours: int = 24,      # æ•°æ®ä¿ç•™æ—¶é—´ï¼ˆå°æ—¶ï¼‰
                 alert_threshold: Optional[AlertThreshold] = None):
        
        self.collection_interval = collection_interval
        self.retention_hours = retention_hours
        self.alert_threshold = alert_threshold or AlertThreshold()
        
        # æ€§èƒ½æ•°æ®å­˜å‚¨ï¼ˆå†…å­˜ä¸­çš„æ—¶é—´åºåˆ—æ•°æ®ï¼‰
        self.metrics_history: deque = deque(maxlen=int(retention_hours * 3600 / collection_interval))
        
        # ç»Ÿè®¡è®¡æ•°å™¨
        self.request_count = 0
        self.error_count = 0
        self.response_times = deque(maxlen=1000)  # æœ€è¿‘1000ä¸ªè¯·æ±‚çš„å“åº”æ—¶é—´
        
        # ç›‘æ§çŠ¶æ€
        self.is_running = False
        self.last_collection_time = None
        
        # å‘Šè­¦çŠ¶æ€
        self.active_alerts = []
        
        logger.info("æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def start_monitoring(self):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        if self.is_running:
            logger.warning("æ€§èƒ½ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return
        
        self.is_running = True
        logger.info(f"ğŸš€ å¯åŠ¨æ€§èƒ½ç›‘æ§ï¼Œæ”¶é›†é—´éš”: {self.collection_interval}ç§’")
        
        try:
            while self.is_running:
                await self._collect_metrics()
                await self._check_alerts()
                await asyncio.sleep(self.collection_interval)
                
        except Exception as e:
            logger.error(f"æ€§èƒ½ç›‘æ§è¿è¡Œå¼‚å¸¸: {e}")
        finally:
            self.is_running = False
            logger.info("æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.is_running = False
        logger.info("æ­£åœ¨åœæ­¢æ€§èƒ½ç›‘æ§...")
    
    async def _collect_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        try:
            start_time = time.time()
            
            # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            # åº”ç”¨æ€§èƒ½æŒ‡æ ‡
            avg_response_time = self._calculate_avg_response_time()
            error_rate = self._calculate_error_rate()
            active_connections = self._get_active_connections()
            
            # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡å¯¹è±¡
            metric = PerformanceMetric(
                timestamp=datetime.now().isoformat(),
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                disk_usage_percent=disk.percent,
                response_time_ms=avg_response_time,
                error_rate=error_rate,
                active_connections=active_connections
            )
            
            # å­˜å‚¨æŒ‡æ ‡
            self.metrics_history.append(metric)
            self.last_collection_time = datetime.now()
            
            # è®°å½•æ”¶é›†è€—æ—¶
            collection_time = (time.time() - start_time) * 1000
            logger.debug(f"æ€§èƒ½æŒ‡æ ‡æ”¶é›†å®Œæˆï¼Œè€—æ—¶: {collection_time:.2f}ms")
            
            # å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®
            await self._cleanup_old_data()
            
        except Exception as e:
            logger.error(f"æ€§èƒ½æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
    
    def _calculate_avg_response_time(self) -> float:
        """è®¡ç®—å¹³å‡å“åº”æ—¶é—´"""
        if not self.response_times:
            return 0.0
        return sum(self.response_times) / len(self.response_times)
    
    def _calculate_error_rate(self) -> float:
        """è®¡ç®—é”™è¯¯ç‡"""
        if self.request_count == 0:
            return 0.0
        return self.error_count / self.request_count
    
    def _get_active_connections(self) -> int:
        """è·å–æ´»è·ƒè¿æ¥æ•°"""
        try:
            # è·å–ç½‘ç»œè¿æ¥ä¿¡æ¯
            connections = psutil.net_connections(kind='inet')
            active_count = len([conn for conn in connections if conn.status == 'ESTABLISHED'])
            return active_count
        except Exception as e:
            logger.warning(f"è·å–è¿æ¥æ•°å¤±è´¥: {e}")
            return 0
    
    async def _check_alerts(self):
        """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
        if not self.metrics_history:
            return
        
        latest_metric = self.metrics_history[-1]
        new_alerts = []
        
        # CPUä½¿ç”¨ç‡å‘Šè­¦
        if latest_metric.cpu_percent > self.alert_threshold.cpu_threshold:
            new_alerts.append({
                "type": "cpu_high",
                "message": f"CPUä½¿ç”¨ç‡è¿‡é«˜: {latest_metric.cpu_percent:.1f}%",
                "severity": "warning",
                "timestamp": latest_metric.timestamp,
                "value": latest_metric.cpu_percent,
                "threshold": self.alert_threshold.cpu_threshold
            })
        
        # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
        if latest_metric.memory_percent > self.alert_threshold.memory_threshold:
            new_alerts.append({
                "type": "memory_high",
                "message": f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {latest_metric.memory_percent:.1f}%",
                "severity": "warning",
                "timestamp": latest_metric.timestamp,
                "value": latest_metric.memory_percent,
                "threshold": self.alert_threshold.memory_threshold
            })
        
        # ç£ç›˜ä½¿ç”¨ç‡å‘Šè­¦
        if latest_metric.disk_usage_percent > self.alert_threshold.disk_threshold:
            new_alerts.append({
                "type": "disk_high",
                "message": f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {latest_metric.disk_usage_percent:.1f}%",
                "severity": "critical",
                "timestamp": latest_metric.timestamp,
                "value": latest_metric.disk_usage_percent,
                "threshold": self.alert_threshold.disk_threshold
            })
        
        # å“åº”æ—¶é—´å‘Šè­¦
        if latest_metric.response_time_ms > self.alert_threshold.response_time_threshold:
            new_alerts.append({
                "type": "response_time_high",
                "message": f"å“åº”æ—¶é—´è¿‡é•¿: {latest_metric.response_time_ms:.1f}ms",
                "severity": "warning",
                "timestamp": latest_metric.timestamp,
                "value": latest_metric.response_time_ms,
                "threshold": self.alert_threshold.response_time_threshold
            })
        
        # é”™è¯¯ç‡å‘Šè­¦
        if latest_metric.error_rate > self.alert_threshold.error_rate_threshold:
            new_alerts.append({
                "type": "error_rate_high",
                "message": f"é”™è¯¯ç‡è¿‡é«˜: {latest_metric.error_rate:.2%}",
                "severity": "critical",
                "timestamp": latest_metric.timestamp,
                "value": latest_metric.error_rate,
                "threshold": self.alert_threshold.error_rate_threshold
            })
        
        # æ›´æ–°æ´»è·ƒå‘Šè­¦
        self.active_alerts = new_alerts
        
        # è®°å½•å‘Šè­¦
        if new_alerts:
            for alert in new_alerts:
                logger.warning(f"âš ï¸ æ€§èƒ½å‘Šè­¦: {alert['message']}")
    
    async def _cleanup_old_data(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        # dequeä¼šè‡ªåŠ¨é™åˆ¶é•¿åº¦ï¼Œè¿™é‡Œä¸»è¦æ˜¯æ¸…ç†å…¶ä»–å¯èƒ½çš„è¿‡æœŸæ•°æ®
        cutoff_time = datetime.now() - timedelta(hours=self.retention_hours)
        
        # æ¸…ç†è¿‡æœŸçš„å“åº”æ—¶é—´è®°å½•ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
        if len(self.response_times) > 1000:
            # ä¿ç•™æœ€è¿‘çš„1000ä¸ªè®°å½•
            while len(self.response_times) > 1000:
                self.response_times.popleft()
    
    def record_request(self, response_time_ms: float, is_error: bool = False):
        """è®°å½•è¯·æ±‚æ€§èƒ½æ•°æ®"""
        self.request_count += 1
        self.response_times.append(response_time_ms)
        
        if is_error:
            self.error_count += 1
        
        logger.debug(f"è®°å½•è¯·æ±‚: å“åº”æ—¶é—´={response_time_ms:.2f}ms, é”™è¯¯={is_error}")
    
    async def get_current_metrics(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡"""
        if not self.metrics_history:
            return {"error": "æš‚æ— æ€§èƒ½æ•°æ®"}
        
        latest_metric = self.metrics_history[-1]
        
        return {
            "current": asdict(latest_metric),
            "alerts": self.active_alerts,
            "monitoring_status": {
                "is_running": self.is_running,
                "last_collection": self.last_collection_time.isoformat() if self.last_collection_time else None,
                "data_points": len(self.metrics_history),
                "collection_interval": self.collection_interval
            }
        }
    
    async def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦æŠ¥å‘Š"""
        if not self.metrics_history:
            return {"error": "æš‚æ— æ€§èƒ½æ•°æ®"}
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        metrics_data = [asdict(m) for m in self.metrics_history]
        
        # æœ€è¿‘1å°æ—¶çš„æ•°æ®
        recent_metrics = [m for m in self.metrics_history 
                         if datetime.fromisoformat(m.timestamp) > datetime.now() - timedelta(hours=1)]
        
        if recent_metrics:
            avg_cpu = sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics)
            avg_memory = sum(m.memory_percent for m in recent_metrics) / len(recent_metrics)
            avg_response_time = sum(m.response_time_ms for m in recent_metrics) / len(recent_metrics)
            max_cpu = max(m.cpu_percent for m in recent_metrics)
            max_memory = max(m.memory_percent for m in recent_metrics)
        else:
            avg_cpu = avg_memory = avg_response_time = max_cpu = max_memory = 0
        
        return {
            "summary": {
                "total_data_points": len(self.metrics_history),
                "recent_hour_points": len(recent_metrics),
                "avg_cpu_1h": round(avg_cpu, 2),
                "avg_memory_1h": round(avg_memory, 2),
                "avg_response_time_1h": round(avg_response_time, 2),
                "max_cpu_1h": round(max_cpu, 2),
                "max_memory_1h": round(max_memory, 2)
            },
            "current_alerts": self.active_alerts,
            "request_stats": {
                "total_requests": self.request_count,
                "total_errors": self.error_count,
                "current_error_rate": self._calculate_error_rate()
            },
            "thresholds": asdict(self.alert_threshold),
            "last_updated": datetime.now().isoformat()
        }
    
    async def get_metrics_history(self, hours: int = 1) -> List[Dict[str, Any]]:
        """è·å–æŒ‡å®šæ—¶é—´èŒƒå›´çš„å†å²æŒ‡æ ‡"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        filtered_metrics = [
            asdict(m) for m in self.metrics_history
            if datetime.fromisoformat(m.timestamp) > cutoff_time
        ]
        
        return filtered_metrics
    
    def export_metrics_to_file(self, file_path: str):
        """å¯¼å‡ºæ€§èƒ½æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        try:
            metrics_data = [asdict(m) for m in self.metrics_history]
            
            export_data = {
                "export_timestamp": datetime.now().isoformat(),
                "total_metrics": len(metrics_data),
                "collection_interval": self.collection_interval,
                "retention_hours": self.retention_hours,
                "metrics": metrics_data
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ€§èƒ½æŒ‡æ ‡å·²å¯¼å‡ºåˆ°: {file_path}")
            
        except Exception as e:
            logger.error(f"æ€§èƒ½æŒ‡æ ‡å¯¼å‡ºå¤±è´¥: {e}")


# å…¨å±€æ€§èƒ½ç›‘æ§å™¨å®ä¾‹
_global_monitor = None


def get_performance_monitor() -> PerformanceMonitor:
    """è·å–å…¨å±€æ€§èƒ½ç›‘æ§å™¨å®ä¾‹"""
    global _global_monitor
    if _global_monitor is None:
        _global_monitor = PerformanceMonitor()
    return _global_monitor


async def start_background_monitoring():
    """å¯åŠ¨åå°æ€§èƒ½ç›‘æ§"""
    monitor = get_performance_monitor()
    await monitor.start_monitoring()


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæ€§èƒ½ç›‘æ§å™¨
    async def main():
        monitor = PerformanceMonitor(collection_interval=10)  # 10ç§’æ”¶é›†ä¸€æ¬¡
        
        # å¯åŠ¨ç›‘æ§
        monitoring_task = asyncio.create_task(monitor.start_monitoring())
        
        # æ¨¡æ‹Ÿä¸€äº›è¯·æ±‚æ•°æ®
        async def simulate_requests():
            import random
            for i in range(100):
                response_time = random.uniform(100, 2000)  # 100msåˆ°2s
                is_error = random.random() < 0.02  # 2%é”™è¯¯ç‡
                monitor.record_request(response_time, is_error)
                await asyncio.sleep(1)
        
        # å¯åŠ¨æ¨¡æ‹Ÿè¯·æ±‚
        simulation_task = asyncio.create_task(simulate_requests())
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ååœæ­¢
        await asyncio.sleep(60)  # è¿è¡Œ1åˆ†é’Ÿ
        monitor.stop_monitoring()
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        await asyncio.gather(monitoring_task, simulation_task, return_exceptions=True)
        
        # è¾“å‡ºæ€§èƒ½æ‘˜è¦
        summary = await monitor.get_performance_summary()
        print(json.dumps(summary, ensure_ascii=False, indent=2))
    
    asyncio.run(main())
