"""
Promptæ€§èƒ½ç›‘æ§å®šæ—¶ä»»åŠ¡

è´Ÿè´£å®šæœŸæ‰§è¡ŒPromptæ€§èƒ½åˆ†æå’Œä¼˜åŒ–ï¼Œæ”¯æŒï¼š
- æ¯æ—¥æ€§èƒ½åˆ†ææŠ¥å‘Š
- è‡ªåŠ¨ç‰ˆæœ¬åˆ‡æ¢å»ºè®®
- å¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦
- æ€§èƒ½è¶‹åŠ¿åˆ†æ
"""

import logging
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import json

from ..services.prompt_optimizer_service import prompt_optimizer_service
from ..services.prompt_version_service import prompt_version_service
from ..services.prompt_performance_service import prompt_performance_service

logger = logging.getLogger(__name__)


class PromptMonitoringTask:
    """Promptæ€§èƒ½ç›‘æ§å®šæ—¶ä»»åŠ¡"""
    
    def __init__(self):
        self.logger = logger
        self.is_running = False
        self.last_analysis_time = None
        
    async def run_daily_analysis(self) -> Dict[str, Any]:
        """
        è¿è¡Œæ¯æ—¥æ€§èƒ½åˆ†æ
        
        Returns:
            åˆ†æç»“æœæ‘˜è¦
        """
        if self.is_running:
            self.logger.warning("Daily analysis is already running, skipping...")
            return {"status": "skipped", "reason": "already_running"}
        
        self.is_running = True
        start_time = datetime.utcnow()
        
        try:
            self.logger.info("Starting daily prompt performance analysis")
            
            # 1. è¿è¡Œæ€§èƒ½åˆ†æ
            analysis_result = await prompt_optimizer_service.run_daily_analysis()
            
            # 2. æ£€æŸ¥ç‰ˆæœ¬æ€§èƒ½å¹¶ç”Ÿæˆåˆ‡æ¢å»ºè®®
            version_recommendations = await self._analyze_version_performance()
            
            # 3. æ£€æµ‹å¼‚å¸¸æƒ…å†µ
            anomalies = await self._detect_anomalies()
            
            # 4. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            report = await self._generate_daily_report(
                analysis_result, 
                version_recommendations, 
                anomalies
            )
            
            # 5. ä¿å­˜æŠ¥å‘Š
            await self._save_daily_report(report)
            
            # 6. å‘é€å‘Šè­¦ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰
            await self._send_alerts_if_needed(report)
            
            self.last_analysis_time = start_time
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            summary = {
                "status": "completed",
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "analysis_result": analysis_result,
                "version_recommendations": version_recommendations,
                "anomalies_detected": len(anomalies),
                "report_generated": True
            }
            
            self.logger.info(f"Daily analysis completed in {duration:.2f} seconds")
            return summary
            
        except Exception as e:
            self.logger.error(f"Error in daily analysis: {e}")
            return {
                "status": "error",
                "error": str(e),
                "start_time": start_time.isoformat(),
                "end_time": datetime.utcnow().isoformat()
            }
        finally:
            self.is_running = False
    
    async def _analyze_version_performance(self) -> List[Dict[str, Any]]:
        """åˆ†æç‰ˆæœ¬æ€§èƒ½å¹¶ç”Ÿæˆåˆ‡æ¢å»ºè®®"""
        try:
            recommendations = []
            scenarios = ["health_advice"]  # å¯æ‰©å±•
            
            for scenario in scenarios:
                # è·å–å½“å‰æœ€ä½³ç‰ˆæœ¬
                current_best = await prompt_version_service.get_best_version(scenario)
                
                # è·å–æ‰€æœ‰ç‰ˆæœ¬çš„æ€§èƒ½æ•°æ®
                versions_data = {}
                for version in ["v3_0", "v3_1", "v3_2_test"]:
                    try:
                        stats = await prompt_performance_service.get_performance_stats(
                            scenario, version, days=7
                        )
                        if stats.get('total_uses', 0) >= 5:  # æœ€å°‘5æ¬¡ä½¿ç”¨
                            versions_data[version] = stats
                    except Exception as e:
                        self.logger.warning(f"Failed to get stats for {version}: {e}")
                
                if len(versions_data) >= 2:
                    # æ‰¾å‡ºæ€§èƒ½æœ€ä½³çš„ç‰ˆæœ¬
                    best_version = max(
                        versions_data.keys(),
                        key=lambda v: (
                            versions_data[v].get('average_rating', 0) * 0.4 +
                            versions_data[v].get('average_relevance', 0) * 0.3 +
                            (1 - versions_data[v].get('error_rate_percent', 100) / 100) * 0.3
                        )
                    )
                    
                    if best_version != current_best:
                        performance_diff = self._calculate_performance_difference(
                            versions_data[current_best],
                            versions_data[best_version]
                        )
                        
                        if performance_diff > 0.1:  # 10%ä»¥ä¸Šçš„æ€§èƒ½æå‡
                            recommendations.append({
                                "scenario": scenario,
                                "current_version": current_best,
                                "recommended_version": best_version,
                                "performance_improvement": f"{performance_diff:.1%}",
                                "reason": "æ˜¾è‘—æ€§èƒ½æå‡",
                                "confidence": "high" if performance_diff > 0.2 else "medium"
                            })
            
            return recommendations
            
        except Exception as e:
            self.logger.error(f"Error analyzing version performance: {e}")
            return []
    
    async def _detect_anomalies(self) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¼‚å¸¸æƒ…å†µ"""
        try:
            anomalies = []
            
            # æ£€æŸ¥é”™è¯¯ç‡å¼‚å¸¸
            stats = await prompt_performance_service.get_performance_stats(
                "health_advice", days=1
            )
            
            if stats.get('error_rate_percent', 0) > 10:
                anomalies.append({
                    "type": "high_error_rate",
                    "severity": "high",
                    "description": f"é”™è¯¯ç‡å¼‚å¸¸é«˜: {stats['error_rate_percent']:.1f}%",
                    "threshold": "10%",
                    "current_value": f"{stats['error_rate_percent']:.1f}%"
                })
            
            # æ£€æŸ¥å“åº”æ—¶é—´å¼‚å¸¸
            if stats.get('average_response_time_ms', 0) > 5000:
                anomalies.append({
                    "type": "slow_response",
                    "severity": "medium",
                    "description": f"å“åº”æ—¶é—´è¿‡æ…¢: {stats['average_response_time_ms']:.0f}ms",
                    "threshold": "5000ms",
                    "current_value": f"{stats['average_response_time_ms']:.0f}ms"
                })
            
            # æ£€æŸ¥ç”¨æˆ·è¯„åˆ†ä¸‹é™
            if stats.get('average_rating', 5) < 3.5:
                anomalies.append({
                    "type": "low_user_rating",
                    "severity": "high",
                    "description": f"ç”¨æˆ·è¯„åˆ†è¿‡ä½: {stats['average_rating']:.1f}",
                    "threshold": "3.5",
                    "current_value": f"{stats['average_rating']:.1f}"
                })
            
            return anomalies
            
        except Exception as e:
            self.logger.error(f"Error detecting anomalies: {e}")
            return []
    
    async def _generate_daily_report(
        self, 
        analysis_result: Dict[str, Any],
        version_recommendations: List[Dict[str, Any]],
        anomalies: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ¯æ—¥æŠ¥å‘Š"""
        
        report = {
            "report_date": datetime.utcnow().isoformat(),
            "report_type": "daily_prompt_performance",
            "summary": {
                "total_scenarios_analyzed": len(analysis_result.get('results', {})),
                "version_recommendations": len(version_recommendations),
                "anomalies_detected": len(anomalies),
                "overall_health": self._calculate_overall_health(analysis_result)
            },
            "analysis_details": analysis_result,
            "version_recommendations": version_recommendations,
            "anomalies": anomalies,
            "action_items": self._generate_action_items(version_recommendations, anomalies)
        }
        
        return report
    
    async def _save_daily_report(self, report: Dict[str, Any]):
        """ä¿å­˜æ¯æ—¥æŠ¥å‘Š"""
        try:
            # è¿™é‡Œå¯ä»¥ä¿å­˜åˆ°æ•°æ®åº“æˆ–æ–‡ä»¶ç³»ç»Ÿ
            # ç›®å‰å…ˆè®°å½•åˆ°æ—¥å¿—
            self.logger.info(f"Daily report generated: {json.dumps(report, indent=2, ensure_ascii=False)}")
        except Exception as e:
            self.logger.error(f"Error saving daily report: {e}")
    
    async def _send_alerts_if_needed(self, report: Dict[str, Any]):
        """å‘é€å‘Šè­¦ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰"""
        try:
            high_severity_anomalies = [
                a for a in report.get('anomalies', []) 
                if a.get('severity') == 'high'
            ]
            
            if high_severity_anomalies:
                alert_message = f"ğŸš¨ Promptæ€§èƒ½å‘Šè­¦\n\næ£€æµ‹åˆ° {len(high_severity_anomalies)} ä¸ªé«˜ä¸¥é‡æ€§å¼‚å¸¸:\n"
                for anomaly in high_severity_anomalies:
                    alert_message += f"- {anomaly['description']}\n"
                
                # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶ã€é’‰é’‰ã€å¾®ä¿¡ç­‰å‘Šè­¦æ¸ é“
                self.logger.warning(f"ALERT: {alert_message}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡è¦çš„ç‰ˆæœ¬åˆ‡æ¢å»ºè®®
            high_confidence_recommendations = [
                r for r in report.get('version_recommendations', [])
                if r.get('confidence') == 'high'
            ]
            
            if high_confidence_recommendations:
                self.logger.info(f"å‘ç° {len(high_confidence_recommendations)} ä¸ªé«˜ç½®ä¿¡åº¦ç‰ˆæœ¬åˆ‡æ¢å»ºè®®")
                
        except Exception as e:
            self.logger.error(f"Error sending alerts: {e}")
    
    def _calculate_performance_difference(self, current_stats: Dict, new_stats: Dict) -> float:
        """è®¡ç®—æ€§èƒ½å·®å¼‚"""
        try:
            current_score = (
                current_stats.get('average_rating', 0) * 0.4 +
                current_stats.get('average_relevance', 0) * 0.3 +
                (1 - current_stats.get('error_rate_percent', 100) / 100) * 0.3
            )
            
            new_score = (
                new_stats.get('average_rating', 0) * 0.4 +
                new_stats.get('average_relevance', 0) * 0.3 +
                (1 - new_stats.get('error_rate_percent', 100) / 100) * 0.3
            )
            
            return (new_score - current_score) / current_score if current_score > 0 else 0
            
        except Exception as e:
            self.logger.error(f"Error calculating performance difference: {e}")
            return 0
    
    def _calculate_overall_health(self, analysis_result: Dict[str, Any]) -> str:
        """è®¡ç®—æ•´ä½“å¥åº·åº¦"""
        try:
            results = analysis_result.get('results', {})
            if not results:
                return "unknown"
            
            health_scores = []
            for scenario_result in results.values():
                if scenario_result.get('status') == 'completed':
                    health = scenario_result.get('overall_health', 'unknown')
                    if health == 'excellent':
                        health_scores.append(4)
                    elif health == 'good':
                        health_scores.append(3)
                    elif health == 'fair':
                        health_scores.append(2)
                    elif health == 'poor':
                        health_scores.append(1)
            
            if not health_scores:
                return "unknown"
            
            avg_score = sum(health_scores) / len(health_scores)
            
            if avg_score >= 3.5:
                return "excellent"
            elif avg_score >= 2.5:
                return "good"
            elif avg_score >= 1.5:
                return "fair"
            else:
                return "poor"
                
        except Exception as e:
            self.logger.error(f"Error calculating overall health: {e}")
            return "unknown"
    
    def _generate_action_items(
        self, 
        version_recommendations: List[Dict[str, Any]], 
        anomalies: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆè¡ŒåŠ¨é¡¹"""
        action_items = []
        
        # åŸºäºç‰ˆæœ¬å»ºè®®ç”Ÿæˆè¡ŒåŠ¨é¡¹
        for rec in version_recommendations:
            if rec.get('confidence') == 'high':
                action_items.append({
                    "priority": "high",
                    "type": "version_switch",
                    "title": f"åˆ‡æ¢åˆ°æ€§èƒ½æ›´ä¼˜çš„ç‰ˆæœ¬ {rec['recommended_version']}",
                    "description": f"åœºæ™¯ {rec['scenario']} å¯è·å¾— {rec['performance_improvement']} çš„æ€§èƒ½æå‡",
                    "estimated_effort": "low"
                })
        
        # åŸºäºå¼‚å¸¸ç”Ÿæˆè¡ŒåŠ¨é¡¹
        for anomaly in anomalies:
            if anomaly.get('severity') == 'high':
                action_items.append({
                    "priority": "high",
                    "type": "fix_anomaly",
                    "title": f"ä¿®å¤{anomaly['type']}é—®é¢˜",
                    "description": anomaly['description'],
                    "estimated_effort": "medium"
                })
        
        return action_items


# å…¨å±€å®ä¾‹
prompt_monitoring_task = PromptMonitoringTask()


# å®šæ—¶ä»»åŠ¡è°ƒåº¦å‡½æ•°
async def schedule_daily_analysis():
    """è°ƒåº¦æ¯æ—¥åˆ†æä»»åŠ¡"""
    try:
        result = await prompt_monitoring_task.run_daily_analysis()
        logger.info(f"Scheduled daily analysis completed: {result['status']}")
        return result
    except Exception as e:
        logger.error(f"Error in scheduled daily analysis: {e}")
        return {"status": "error", "error": str(e)}
