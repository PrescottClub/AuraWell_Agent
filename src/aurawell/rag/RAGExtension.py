"""
å¢å¼ºæ£€ç´¢æ–¹æ³•ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼åº¦æŸ¥æ‰¾ï¼Œæ£€ç´¢æ•°æ®åº“ä¸­å‰Kä¸ªå’Œç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„å­—æ®µï¼Œä¹Ÿæä¾›å°†æ–‡æ¡£è§£æå¹¶ä¸Šä¼ è‡³å‘é‡æ•°æ®åº“çš„æ–¹æ³•
è¯¥æ‹“å±•ä½¿ç”¨é˜¿é‡Œå·´å·´æä¾›çš„äº‘æœåŠ¡ï¼Œä½¿ç”¨å‰è¯·åŠ¡å¿…é€šè¿‡pipå‘½ä»¤å®‰è£…å¥½å¯¹åº”çš„åº“
ç›®å‰é€šè¿‡.envè§£æå¯†é’¥ï¼Œæœªæ¥å°†é€šè¿‡KMSå¯†é’¥ç®¡ç†ç³»ç»Ÿè§£æå¯†é’¥
rag_utilsæ˜¯ä¸€ä¸ªè¾…åŠ©RAGExtensionæ ¸å¿ƒé€»è¾‘è¿è¡Œçš„å·¥å…·ç±»ï¼Œå…¶æœ¬èº«ä¸ä¼šè°ƒç”¨API
"""
try:
    from .rag_utils import get_file_type, process_list
    from .oss_utils import OSSManager
    from .file_index_manager import FileIndexManager
except ImportError:
    from rag_utils import get_file_type, process_list
    from oss_utils import OSSManager
    from file_index_manager import FileIndexManager
from alibabacloud_docmind_api20220711.client import Client as docmind_api20220711Client
from alibabacloud_tea_openapi import models as open_api_models
from alibabacloud_docmind_api20220711 import models as docmind_api20220711_models
from alibabacloud_tea_util import models as util_models
from alibabacloud_credentials.client import Client as CredClient
from alibabacloud_credentials.models import Config as CredentialsConfig
from dashvector import Doc

from openai import OpenAI
from dotenv import load_dotenv
import uuid
import os
import time
import dashvector
import numpy as np
import platform
import re
import math
import tempfile
from datetime import datetime, timedelta, timezone

def normalize_file_path(file_path: str) -> str:
    """
    è·¨å¹³å°æ–‡ä»¶è·¯å¾„æ ‡å‡†åŒ–å¤„ç†

    Args:
        file_path (str): è¾“å…¥çš„æ–‡ä»¶è·¯å¾„

    Returns:
        str: æ ‡å‡†åŒ–åçš„ç»å¯¹è·¯å¾„
    """
    # å°†è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    abs_path = os.path.abspath(file_path)

    # æ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦
    normalized_path = os.path.normpath(abs_path)

    return normalized_path

def load_api_keys():
    """
    ä».envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­åŠ è½½APIå¯†é’¥
    ä¼˜å…ˆå°è¯•ä».envæ–‡ä»¶åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½

    Returns:
        dict: åŒ…å«æ‰€æœ‰APIå¯†é’¥çš„å­—å…¸ï¼Œå¦‚æœæŸä¸ªå¯†é’¥æœªæ‰¾åˆ°åˆ™å€¼ä¸ºNone
        bool: æ˜¯å¦æˆåŠŸåŠ è½½äº†æ‰€æœ‰å¿…éœ€çš„å¯†é’¥
    """
    keys = {
        "ALIBABA_CLOUD_ACCESS_KEY_ID": None,
        "ALIBABA_CLOUD_ACCESS_KEY_SECRET": None,
        "DASHSCOPE_API_KEY": None,
        "ALIBABA_QWEN_API_KEY": None,
        "DASH_VECTOR_API": None
    }

    # æ–¹æ³•1: å°è¯•ä».envæ–‡ä»¶åŠ è½½
    try:
        # è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        current_file_path = os.path.abspath(__file__)
        # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆå³ src/aurawell/ragï¼‰
        current_dir = os.path.dirname(current_file_path)
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‘ä¸Šä¸‰çº§ï¼šrag -> aurawell -> src -> é¡¹ç›®æ ¹ç›®å½•ï¼‰
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
        # æ„å»º .env æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        dotenv_path = os.path.join(project_root, '.env')

        # å¦‚æœ.envæ–‡ä»¶å­˜åœ¨ï¼Œå°è¯•åŠ è½½
        if os.path.exists(dotenv_path):
            load_dotenv(dotenv_path=dotenv_path)
            print(f"âœ… å°è¯•ä».envæ–‡ä»¶åŠ è½½å¯†é’¥: {dotenv_path}")
        else:
            print(f"âš ï¸  .envæ–‡ä»¶ä¸å­˜åœ¨: {dotenv_path}")
    except Exception as e:
        print(f"âš ï¸  ä».envæ–‡ä»¶åŠ è½½å¯†é’¥å¤±è´¥: {e}")

    # æ–¹æ³•2: ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆåŒ…æ‹¬.envåŠ è½½åçš„ç¯å¢ƒå˜é‡ï¼‰
    for key in keys.keys():
        value = os.getenv(key)
        if value:
            keys[key] = value
            print(f"âœ… æˆåŠŸåŠ è½½å¯†é’¥: {key}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°å¯†é’¥: {key}")

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…éœ€çš„å¯†é’¥éƒ½å·²åŠ è½½
    missing_keys = [key for key, value in keys.items() if not value]
    success = len(missing_keys) == 0

    if success:
        print("ğŸ‰ æ‰€æœ‰APIå¯†é’¥åŠ è½½æˆåŠŸ")
    else:
        print(f"âš ï¸  ç¼ºå°‘ä»¥ä¸‹å¯†é’¥: {missing_keys}")

    return keys, success

def detect_language(text: str) -> str:
    """
    æ£€æµ‹æ–‡æœ¬è¯­è¨€ï¼Œè¿”å› 'chinese' æˆ– 'english'
    å¦‚æœä¸ç¡®å®šåˆ™é»˜è®¤è¿”å› 'chinese'

    Args:
        text (str): å¾…æ£€æµ‹çš„æ–‡æœ¬

    Returns:
        str: 'chinese' æˆ– 'english'
    """
    if not text or not isinstance(text, str):
        return 'chinese'

    # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    # ç»Ÿè®¡è‹±æ–‡å­—ç¬¦æ•°é‡ï¼ˆå­—æ¯ï¼‰
    english_chars = len(re.findall(r'[a-zA-Z]', text))

    # å¦‚æœä¸­æ–‡å­—ç¬¦å æ¯”è¶…è¿‡30%ï¼Œè®¤ä¸ºæ˜¯ä¸­æ–‡
    total_chars = len(text.strip())
    if total_chars == 0:
        return 'chinese'

    chinese_ratio = chinese_chars / total_chars
    english_ratio = english_chars / total_chars

    # å¦‚æœä¸­æ–‡å­—ç¬¦æ¯”ä¾‹å¤§äºè‹±æ–‡å­—ç¬¦æ¯”ä¾‹ï¼Œä¸”ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹è¶…è¿‡0.1ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
    if chinese_ratio > english_ratio and chinese_ratio > 0.1:
        return 'chinese'
    elif english_ratio > 0.3:  # è‹±æ–‡å­—ç¬¦æ¯”ä¾‹è¶…è¿‡30%è®¤ä¸ºæ˜¯è‹±æ–‡
        return 'english'
    else:
        return 'chinese'  # é»˜è®¤è¿”å›ä¸­æ–‡

def translate_text(text: str, target_language: str, api_key: str, base_url: str) -> str:
    """
    ä½¿ç”¨é˜¿é‡Œäº‘å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç¿»è¯‘

    Args:
        text (str): å¾…ç¿»è¯‘çš„æ–‡æœ¬
        target_language (str): ç›®æ ‡è¯­è¨€ ('chinese' æˆ– 'english')
        api_key (str): APIå¯†é’¥
        base_url (str): APIåŸºç¡€URL

    Returns:
        str: ç¿»è¯‘åçš„æ–‡æœ¬
    """
    try:
        client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )

        if target_language == 'chinese':
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‹±è¯‘ä¸­ç¿»è¯‘ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·è¾“å…¥çš„è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡ï¼Œä¿æŒåŸæ„ä¸å˜ã€‚åªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚"
            user_prompt = f"è¯·å°†ä»¥ä¸‹è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼š{text}"
        else:  # target_language == 'english'
            system_prompt = "You are a professional Chinese-to-English translator. Please translate the user's Chinese text into English while maintaining the original meaning. Only return the translation result without any explanations."
            user_prompt = f"è¯·å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆè‹±æ–‡ï¼š{text}"

        completion = client.chat.completions.create(
            model="qwen-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            max_tokens=1024
        )

        translated_text = completion.choices[0].message.content.strip()
        return translated_text

    except Exception as e:
        print(f"ç¿»è¯‘å¤±è´¥: {e}")
        return text  # ç¿»è¯‘å¤±è´¥æ—¶è¿”å›åŸæ–‡

class Document:
    def __init__(self):
        """
        ä».envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­åŠ è½½APIå¯†é’¥ï¼Œè¿™ä¸ªç±»éœ€è¦è¯»å–çš„api keyæœ‰ï¼š
        ALIBABA_CLOUD_ACCESS_KEY_ID=SECRET
        ALIBABA_CLOUD_ACCESS_KEY_SECRET=SECRET
        DASHSCOPE_API_KEY=SECRET
        DASH_VECTOR_API=SECRET

        ä¼˜å…ˆä».envæ–‡ä»¶åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½
        """
        print("ğŸ”„ Documentç±»åˆå§‹åŒ–ï¼šå¼€å§‹åŠ è½½APIå¯†é’¥...")

        # åŠ è½½APIå¯†é’¥
        keys, success = load_api_keys()

        if not success:
            raise ValueError("âŒ æ— æ³•åŠ è½½å¿…éœ€çš„APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è®¾ç½®")

        # è®¾ç½®å®ä¾‹å˜é‡
        self.access_key_id = keys["ALIBABA_CLOUD_ACCESS_KEY_ID"]
        self.access_key_secret = keys["ALIBABA_CLOUD_ACCESS_KEY_SECRET"]
        self.dash_scope_key = keys["DASHSCOPE_API_KEY"]
        self.qwen_api_key = keys["ALIBABA_QWEN_API_KEY"] or keys["DASHSCOPE_API_KEY"]  # ä¼˜å…ˆä½¿ç”¨QWENå¯†é’¥ï¼Œå›é€€åˆ°DASHSCOPE
        self.dash_vector_key = keys["DASH_VECTOR_API"]

        # è®¿é—®çš„åŸŸå
        # é˜¿é‡Œäº‘docmindæœåŠ¡çš„èŠ‚ç‚¹åœ°å€ï¼Œå†™æˆç±»å±æ€§æ˜¯ä¸ºäº†æ–¹ä¾¿æœªæ¥æ›´æ¢
        self.docmind_endpoint = f'docmind-api.cn-hangzhou.aliyuncs.com'
        # é˜¿é‡Œäº‘çš„å‘é‡æ•°æ®åº“æœåŠ¡çš„èŠ‚ç‚¹åœ°å€
        self.vectorDB_endpoint = "vrs-cn-6sa4axaiv0001c.dashvector.cn-shanghai.aliyuncs.com"
        self.bailian_endpoint = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.region_id = "cn-hangzhou"

        print("âœ… Documentç±»åˆå§‹åŒ–å®Œæˆ")
    def __doc_analysation(self, file_path:str):
        """
        ä½¿ç”¨é˜¿é‡Œäº‘çš„docmindæœåŠ¡å¯¹æ–‡æ¡£è¿›è¡Œè§£æï¼Œè¿”å›è§£æç»“æœ
        :param file_path: æ–‡æ¡£è·¯å¾„
        :return: è§£æç»“æœ
        """
        # æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿è·¨å¹³å°å…¼å®¹æ€§
        normalized_path = normalize_file_path(file_path)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(normalized_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {normalized_path}")

        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
        file_extension = get_file_type(normalized_path)
        supported_formats = ["pdf", "docx", "xlsx"]
        if file_extension not in supported_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}. ä»…æ”¯æŒ: {supported_formats}")
        # æ„é€  Credentialï¼Œç›´æ¥ä»æœ¬åœ°è¯»å–ï¼Œåœ¨æ­£å¼éƒ¨ç½²æ—¶éœ€è¦æ›´æ”¹
        cred_config = CredentialsConfig(
            type="access_key",
            access_key_id=self.access_key_id,
            access_key_secret=self.access_key_secret
        )
        # åŠ è½½å®Œæ¯•ï¼Œç°åœ¨å¯ä»¥åˆå§‹åŒ–ç±»
        cred = CredClient(config=cred_config)
        config = open_api_models.Config(
            # é€šè¿‡credentialsè·å–é…ç½®ä¸­çš„AccessKey ID
            access_key_id=cred.get_credential().get_access_key_id(),
            # é€šè¿‡credentialsè·å–é…ç½®ä¸­çš„AccessKey Secret
            access_key_secret=cred.get_credential().get_access_key_secret()
        )
        config.read_timeout = 2400 # 10é¡µä»¥ä¸Šçš„æ–‡ä»¶å¯èƒ½çœŸçš„è¦ä¸Šä¼ 30åˆ†é’Ÿä»¥ä¸Šï¼Œå®åœ¨æ˜¯å¾ˆéš¾ç»·
        config.connect_timeout = 1200 # äººåœ¨ç€æ–¯ï¼Œè¿æ¥ä¸ç¨³å®šï¼Œæ‰€ä»¥åªèƒ½å…ˆè¿™ä¹ˆè®¾ç½®
        config.endpoint = self.docmind_endpoint
        client = docmind_api20220711Client(config)
        request = docmind_api20220711_models.SubmitDocParserJobAdvanceRequest(
            # file_url_object : æœ¬åœ°æ–‡ä»¶æµ
            file_url_object=open(normalized_path, "rb"),
            # æ–‡æ¡£ç±»å‹æ˜¯å¿…é¡»ä¸Šä¼ çš„å‚æ•°
            file_name_extension=get_file_type(normalized_path)
        )
        runtime = util_models.RuntimeOptions(
            read_timeout=24000,
            connect_timeout=12000
        )
        response = client.submit_doc_parser_job_advance(request, runtime)
        print(response.body) # æŸ¥çœ‹åˆ°åº•æ˜¯ä»€ä¹ˆè¿”å›

        # æ£€æŸ¥å“åº”æ˜¯å¦æˆåŠŸ
        if response.body.data is None:
            print(f"Error: {response.body}")
            raise Exception(f"Document parsing failed: {response.body}")

        job_id = response.body.data.id
        request = docmind_api20220711_models.QueryDocParserStatusRequest(
            # id :  ä»»åŠ¡æäº¤æ¥å£è¿”å›çš„id
            id=job_id
        )
        response = client.query_doc_parser_status(request)
        service_status = response.body.data.status
        wait_time = 0.0 # ç›®å‰æ²¡æœ‰æƒ³åˆ°ä¸€ä¸ªåˆé€‚çš„æ£€æµ‹è¶…æ—¶çš„åŠæ³•ï¼Œå…ˆå°±è¿™ä¹ˆå°†å°±ä¸€ä¸‹å§
        while service_status != "success" and wait_time < 2400:
            time.sleep(0.5)
            wait_time += 0.5
            response = client.query_doc_parser_status(request)
            service_status = response.body.data.status
            print(service_status)
        request = docmind_api20220711_models.GetDocParserResultRequest(
            # id :  ä»»åŠ¡æäº¤æ¥å£è¿”å›çš„id
            id=job_id,
            layout_step_size=100,
            layout_num=0
        )
        # å¤åˆ¶ä»£ç è¿è¡Œè¯·è‡ªè¡Œæ‰“å° API çš„è¿”å›å€¼
        response = client.get_doc_parser_result(request)
        # APIè¿”å›å€¼æ ¼å¼å±‚çº§ä¸º body -> data -> å…·ä½“å±æ€§ã€‚å¯æ ¹æ®ä¸šåŠ¡éœ€è¦æ‰“å°ç›¸åº”çš„ç»“æœã€‚è·å–å±æ€§å€¼å‡ä»¥å°å†™å¼€å¤´
        # è·å–è¿”å›ç»“æœã€‚å»ºè®®å…ˆæŠŠresponse.body.dataè½¬æˆjsonï¼Œç„¶åå†ä»jsoné‡Œé¢å–å…·ä½“éœ€è¦çš„å€¼ã€‚
        print(response.body)
        return response.body.data
    def file_Parsing(self, file_path:str)->str:
        """
        ä»é˜¿é‡Œäº‘ DocMind API è¿”å›çš„ data å­—æ®µä¸­æå–æ‰€æœ‰ markdownContentï¼Œ
        æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„ Markdown å­—ç¬¦ä¸²ï¼Œå¹¶ï¼š
        - å»é™¤ç©ºç™½è¡Œ
        - åˆå¹¶è¿ç»­çš„æ–‡æœ¬å—
        """
        raw_text = self.__doc_analysation(file_path)
        full_markdown = ""
        layouts = raw_text.get("layouts",  []) if raw_text.get("layouts",  None) is not None else []
        for layout in layouts:
            markdown = layout.get("markdownContent", None) if layout.get("markdownContent", None) is not None else ""
            full_markdown += markdown
        # æŒ‰è¡Œåˆ†å‰²
        lines = full_markdown.splitlines()
        cleaned_lines = []
        buffer = ""
        for line in lines:
            stripped_line = line.strip()
            if stripped_line == "":  # ç©ºè¡Œ
                if buffer:
                    cleaned_lines.append(buffer)
                    buffer = ""
            else:
                buffer += " " + stripped_line if buffer else stripped_line
        # æ·»åŠ æœ€åå¯èƒ½å­˜åœ¨çš„æ®µè½
        if buffer:
            cleaned_lines.append(buffer)
        return "\n".join(cleaned_lines)

    def __is_reference_content(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºæ–‡çŒ®å¼•ç”¨å†…å®¹

        Args:
            text (str): å¾…æ£€æµ‹çš„æ–‡æœ¬

        Returns:
            bool: True å¦‚æœæ˜¯å¼•ç”¨å†…å®¹ï¼ŒFalse å¦åˆ™
        """
        if not text or not isinstance(text, str):
            return False

        text = text.strip()

        # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œä¸å¤ªå¯èƒ½æ˜¯å¼•ç”¨
        if len(text) < 10:
            return False

        # æ£€æµ‹å¼•ç”¨çš„ç‰¹å¾æ¨¡å¼
        import re

        # æ¨¡å¼1: ä»¥æ–¹æ‹¬å·æ•°å­—å¼€å¤´çš„å¼•ç”¨ [1], [19], [1-3] ç­‰
        bracket_number_pattern = r'^\[\d+(-\d+)?\]'
        if re.match(bracket_number_pattern, text):
            return True

        # æ¨¡å¼2: åŒ…å«DOIçš„å¼•ç”¨
        doi_pattern = r'DOI\s*:\s*10\.\d+/'
        if re.search(doi_pattern, text, re.IGNORECASE):
            return True

        # æ¨¡å¼3: åŒ…å«æœŸåˆŠå¼•ç”¨æ ¼å¼çš„æ–‡æœ¬
        # æ£€æµ‹æ ¼å¼ï¼šä½œè€…å,æ ‡é¢˜[J].æœŸåˆŠå,å¹´ä»½,å·(æœŸ):é¡µç 
        journal_pattern = r'[^,]+,.*?\[J\]\..*?,\d{4},.*?:'
        if re.search(journal_pattern, text):
            return True

        # æ¨¡å¼4: åŒ…å«å¤šä¸ªä½œè€…çš„å¼•ç”¨æ ¼å¼ (å§“å,å§“å,ç­‰.)
        multiple_authors_pattern = r'[^,]+,[^,]+,ç­‰\.'
        if re.search(multiple_authors_pattern, text):
            return True

        # æ¨¡å¼5: åŒ…å«è‹±æ–‡æœŸåˆŠå¼•ç”¨æ ¼å¼
        # æ£€æµ‹æ ¼å¼ï¼šAuthor A, Author B, et al. Title[J]. Journal, Year, Volume(Issue): Pages.
        english_journal_pattern = r'[A-Z][a-z]+\s+[A-Z]{1,3},.*?et\s+al\..*?\[J\]\..*?,\s*\d{4}'
        if re.search(english_journal_pattern, text):
            return True

        # æ¨¡å¼6: åŒ…å«ISBNæˆ–ISSNçš„å¼•ç”¨
        isbn_issn_pattern = r'(ISBN|ISSN)\s*[:ï¼š]\s*[\d-]+'
        if re.search(isbn_issn_pattern, text, re.IGNORECASE):
            return True

        # æ¨¡å¼7: æ£€æµ‹å¼•ç”¨ä¸­å¸¸è§çš„æ ‡ç‚¹ç¬¦å·å¯†åº¦
        # å¼•ç”¨é€šå¸¸åŒ…å«å¤§é‡çš„é€—å·ã€å¥å·ã€å†’å·ç­‰
        punctuation_count = sum(1 for char in text if char in ',.;:()[]{}')
        if len(text) > 50 and punctuation_count / len(text) > 0.15:
            # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½
            year_pattern = r'\b(19|20)\d{2}\b'
            if re.search(year_pattern, text):
                return True

        return False

    def get_recent_files_from_oss(self, days: int = 30) -> list:
        """
        ä»OSSäº‘å­˜å‚¨ä¸­è¯»å–åœ¨æŒ‡å®šå¤©æ•°å†…ä¸Šä¼ çš„æ–‡ä»¶

        Args:
            days (int): å¤©æ•°ï¼Œé»˜è®¤30å¤©

        Returns:
            list: æ–‡ä»¶è®°å½•åˆ—è¡¨
        """
        try:
            file_manager = FileIndexManager()
            recent_files = file_manager.get_files_uploaded_in_days(days)

            print(f"âœ… ä»OSSè·å–åˆ° {len(recent_files)} ä¸ªåœ¨ {days} å¤©å†…ä¸Šä¼ çš„æ–‡ä»¶")
            return recent_files

        except Exception as e:
            print(f"âŒ ä»OSSè·å–æœ€è¿‘æ–‡ä»¶å¤±è´¥: {e}")
            return []

    def download_file_from_oss(self, oss_key: str, local_path: str = None) -> str:
        """
        ä»OSSä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ä¸´æ—¶ä½ç½®

        Args:
            oss_key (str): OSSä¸­çš„æ–‡ä»¶é”®å
            local_path (str): æœ¬åœ°ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶

        Returns:
            str: æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            oss_manager = OSSManager()

            if local_path is None:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                suffix = os.path.splitext(oss_key)[1] or '.tmp'
                temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
                local_path = temp_file.name
                temp_file.close()

            success = oss_manager.download_file(oss_key, local_path)

            if success:
                print(f"âœ… æ–‡ä»¶ä»OSSä¸‹è½½æˆåŠŸ: {oss_key} -> {local_path}")
                return local_path
            else:
                print(f"âŒ æ–‡ä»¶ä»OSSä¸‹è½½å¤±è´¥: {oss_key}")
                return None

        except Exception as e:
            print(f"âŒ ä»OSSä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
            return None

    def upload_parsed_content_to_oss(self, content: str, filename: str) -> bool:
        """
        å°†è§£æçš„æ–‡ä»¶å†…å®¹ä»¥markdownå½¢å¼ä¸Šä¼ åˆ°OSS

        Args:
            content (str): è§£æåçš„æ–‡æ¡£å†…å®¹
            filename (str): åŸå§‹æ–‡ä»¶å

        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        try:
            oss_manager = OSSManager()

            # æ„å»ºmarkdownæ–‡ä»¶çš„OSSé”®å
            base_name = os.path.splitext(filename)[0]
            markdown_key = f"parsed_content/{base_name}.md"

            # æ·»åŠ æ—¶é—´æˆ³å’Œå…ƒæ•°æ®
            timestamp = datetime.now(timezone(timedelta(hours=8))).strftime("%Y-%m-%d %H:%M:%S")
            markdown_content = f"""# è§£ææ–‡æ¡£: {filename}

**è§£ææ—¶é—´**: {timestamp}
**åŸå§‹æ–‡ä»¶**: {filename}

---

{content}
"""

            success = oss_manager.upload_string_as_file(markdown_content, markdown_key)

            if success:
                print(f"âœ… è§£æå†…å®¹ä¸Šä¼ åˆ°OSSæˆåŠŸ: {markdown_key}")
            else:
                print(f"âŒ è§£æå†…å®¹ä¸Šä¼ åˆ°OSSå¤±è´¥: {markdown_key}")

            return success

        except Exception as e:
            print(f"âŒ ä¸Šä¼ è§£æå†…å®¹åˆ°OSSå¤±è´¥: {e}")
            return False

    def content_filter(self, file_path: str, is_oss_key: bool = False) -> list:
        """
        ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹åˆ†ææ–‡æ¡£å†…å®¹ï¼Œæå–é«˜ä¿¡æ¯å¯†åº¦æ–‡æœ¬ï¼Œå¹¶å°†ç»“æœå‘é‡åŒ–å­˜å‚¨åˆ°æ•°æ®åº“

        Args:
            file_path (str): æ–‡æ¡£è·¯å¾„æˆ–OSSé”®å
            is_oss_key (bool): æ˜¯å¦ä¸ºOSSé”®å

        Returns:
            list: åŒ…å«æå–çš„é«˜å¯†åº¦ä¿¡æ¯æ®µè½çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²
        """
        temp_file_path = None
        try:
            # 1. å¦‚æœæ˜¯OSSé”®åï¼Œå…ˆä¸‹è½½åˆ°æœ¬åœ°
            if is_oss_key:
                temp_file_path = self.download_file_from_oss(file_path)
                if not temp_file_path:
                    print(f"âŒ æ— æ³•ä»OSSä¸‹è½½æ–‡ä»¶: {file_path}")
                    return []
                actual_file_path = temp_file_path
                original_filename = os.path.basename(file_path)
            else:
                actual_file_path = file_path
                original_filename = os.path.basename(file_path)

            # 2. ä½¿ç”¨ç°æœ‰æ–¹æ³•è§£ææ–‡æ¡£
            raw_content = self.__doc_analysation(actual_file_path)

            # 3. æå–æ–‡æ¡£çš„å®Œæ•´æ–‡æœ¬å†…å®¹
            full_text = self.file_Parsing(actual_file_path)

            if not full_text or len(full_text.strip()) < 10:
                print("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œæ— æ³•è¿›è¡Œå†…å®¹è¿‡æ»¤")
                return []

            # 3. ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹æå–é«˜å¯†åº¦ä¿¡æ¯
            client = OpenAI(
                api_key=self.qwen_api_key,
                base_url=self.bailian_endpoint
            )

            # æ„å»ºæç¤ºè¯
            system_prompt = """ä½ æ˜¯ä¸€ä½åŒ»å­¦æ–‡çŒ®ä¿¡æ¯æå–ä¸“å®¶ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹æ–‡æœ¬ï¼Œä»ä¸­æå–æ‰€æœ‰**é«˜å¯†åº¦ä¿¡æ¯æ®µè½**ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼š

- è†³é£Ÿå»ºè®®
- æ¨èæ‘„å…¥é‡
- å¥åº·å»ºè®®
- å…·ä½“é£Ÿç‰©ç§ç±»ä¸å…‹æ•°
- é¥®æ°´ä¸è¿åŠ¨æŒ‡å¯¼
- è¥å…»æ ‡å‡†ä¸æŒ‡æ ‡
- é£Ÿå“æ ‡ç­¾è§£è¯»è¦ç‚¹
- åˆ†é¤ä¸å«ç”Ÿå»ºè®®
- è†³é£Ÿå®å¡”ç»“æ„
- ç‰¹å®šäººç¾¤æ¨èï¼ˆå¦‚å­•å¦‡ã€å„¿ç«¥ã€è€å¹´äººï¼‰

æå–è¦æ±‚å¦‚ä¸‹ï¼š

1. **é«˜å¯†åº¦ä¿¡æ¯**ï¼šå…·å¤‡æ˜ç¡®æ•°å€¼ã€æŒ‡å¯¼å»ºè®®ã€å¯æ‰§è¡Œå†…å®¹
2. **å»é™¤ä½å¯†åº¦ä¿¡æ¯**ï¼šå¦‚èƒŒæ™¯ä»‹ç»ã€å®šä¹‰ã€æ”¿ç­–è¯´æ˜ã€æ„ä¹‰é˜è¿°ç­‰
3. **ä¿ç•™åŸæ–‡æ ¼å¼**ï¼Œä½†åªæå–æ ¸å¿ƒæ®µè½
4. ä½¿ç”¨ `;;` åˆ†éš”æ¯ä¸€ä¸ªæå–å‡ºæ¥çš„ä¿¡æ¯æ®µ

ç°åœ¨è¯·å¤„ç†ä»¥ä¸‹æ–‡æœ¬ï¼š"""

            user_prompt = full_text

            # è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹
            completion = client.chat.completions.create(
                model="qwen-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.3,
                top_p=0.9,
                max_tokens=2048
            )

            # 4. å¤„ç†å¤§è¯­è¨€æ¨¡å‹çš„è¿”å›ç»“æœ
            llm_response = completion.choices[0].message.content.strip()

            if not llm_response:
                print("å¤§è¯­è¨€æ¨¡å‹è¿”å›ç©ºç»“æœ")
                return []

            # 5. æŒ‰ ";;" åˆ†å‰²æ–‡æœ¬
            segments = llm_response.split(";;")

            # 6. æ¸…ç†å’Œè¿‡æ»¤åˆ†å‰²åçš„æ–‡æœ¬æ®µ
            filtered_segments = []
            for segment in segments:
                cleaned_segment = segment.strip()
                # è¿‡æ»¤æ‰é•¿åº¦å°äº3çš„æ®µè½
                if len(cleaned_segment) >= 3:
                    filtered_segments.append(cleaned_segment)

            print(f"æˆåŠŸæå– {len(filtered_segments)} ä¸ªé«˜å¯†åº¦ä¿¡æ¯æ®µè½")

            # 7. å°†è¿‡æ»¤åçš„æ®µè½å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
            if filtered_segments:
                self._vectorize_and_store_segments(filtered_segments)

            # 8. å¦‚æœæ˜¯OSSæ–‡ä»¶ï¼Œä¸Šä¼ è§£æå†…å®¹
            if is_oss_key and full_text:
                try:
                    self.upload_parsed_content_to_oss(full_text, original_filename)
                except Exception as e:
                    print(f"âš ï¸  ä¸Šä¼ è§£æå†…å®¹å¤±è´¥: {e}")

            return filtered_segments

        except Exception as e:
            print(f"âŒ å†…å®¹è¿‡æ»¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return []
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.unlink(temp_file_path)
                    print(f"ğŸ—‘ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file_path}")
                except Exception as e:
                    print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

    def _vectorize_and_store_segments(self, segments: list):
        """
        å°†æ–‡æœ¬æ®µè½å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“

        Args:
            segments (list): æ–‡æœ¬æ®µè½åˆ—è¡¨
        """
        try:
            client = OpenAI(
                api_key=self.dash_scope_key,
                base_url=self.bailian_endpoint
            )

            # è¿æ¥å‘é‡æ•°æ®åº“
            vector_client = dashvector.Client(
                api_key=self.dash_vector_key,
                endpoint=self.vectorDB_endpoint
            )
            collection = vector_client.get(name='simple_collection')

            # æ‰¹é‡å¤„ç†å‘é‡åŒ–ï¼ˆæ¯æ¬¡æœ€å¤š10ä¸ªï¼‰
            for i in range(0, len(segments), 10):
                batch = segments[i:i+10]

                # å‘é‡åŒ–å½“å‰æ‰¹æ¬¡
                completion = client.embeddings.create(
                    model="text-embedding-v4",
                    input=batch,
                    dimensions=1024,
                    encoding_format="float"
                )

                # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                for j, embedding_data in enumerate(completion.data):
                    segment_text = batch[j]
                    vector = np.array(embedding_data.embedding, dtype=float)

                    ret = collection.insert(
                        Doc(
                            id=str(uuid.uuid4()),
                            vector=vector,
                            fields={
                                "raw_text": segment_text,
                                "sub_title": "filtered_content"
                            }
                        )
                    )

                    if not ret:
                        print(f"å‘é‡å­˜å‚¨å¤±è´¥: {segment_text[:50]}...")

            print(f"æˆåŠŸå°† {len(segments)} ä¸ªæ®µè½å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°æ•°æ®åº“")

        except Exception as e:
            print(f"å‘é‡åŒ–å’Œå­˜å‚¨è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    def __content_vectorised(self, raw_content):
        # raw_content å¯¹åº”çš„æ˜¯æ–‡æ¡£è§£ææ–¹æ³•è¿”å›çš„ response.body.data
        discard_type = () # è¿”å›å€¼ä¸­ï¼Œå¿½ç•¥å…¶ä¸­typeå¯¹åº”çš„å€¼ä¸­ä¸ºè¿™äº›å€¼çš„å†…å®¹ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ä¸å¯¹åº”ä»»ä½•å…·ä½“å†…å®¹
        discard_subtype = ("doc_title", "title", "para_title")  # è¿”å›å€¼ä¸­ï¼Œå¿½ç•¥å…¶ä¸­sub_titleå¯¹åº”çš„å€¼ä¸­ä¸ºè¿™äº›å€¼çš„å†…å®¹ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ä¸å¯¹åº”ä»»ä½•å…·ä½“å†…å®¹
        # raw_content æŒ‡çš„æ˜¯PDFæ–‡æ¡£è§£ææœåŠ¡è¿”å›å€¼ä¸­çš„dataå­—æ®µï¼Œå…¶ä½™çš„è¾“å…¥éƒ½ä¼šå¯¼è‡´é”™è¯¯
        query_list = []
        layouts = raw_content.get("layouts", []) if raw_content.get("layouts", None) is not None else []
        for layout in layouts:
            if layout.get("subType", "Unknown") in discard_subtype:
                continue
            elif layout.get("type", "Unknown") == "table": # å¯¹è¿”å›å†…å®¹ä¸­è¢«æ ‡è®°ä¸ºtableçš„å…ƒç´ è¿›è¡Œç‰¹åˆ«å¤„ç†ï¼Œä»¥å»é™¤åœ¨è¯†åˆ«è¿‡ç¨‹ä¸­äº§ç”Ÿçš„éé¢„æœŸå­—ç¬¦
                table_content = layout.get("markdownContent", "")
                table_content = table_content.split("\n")
                table_content = process_list(table_content)
                for s in table_content:
                    if type(s) == type("hello"):
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå¼•ç”¨å†…å®¹ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
                        if not self.__is_reference_content(s):
                            query_list.append(s) # å°†è¡¨æ ¼ä¸­çš„æ¯ä¸€ä¸ªæ ¼å­ä¸­çš„å†…å®¹åŠ å…¥è‡³åˆ—è¡¨ä¸­
                    else:
                        continue

            markdown = layout.get("markdownContent", None) if layout.get("markdownContent", None) is not None else ""
            # æ£€æŸ¥markdownå†…å®¹æ˜¯å¦ä¸ºå¼•ç”¨ï¼Œå¦‚æœä¸æ˜¯å¼•ç”¨æ‰æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            if markdown and not self.__is_reference_content(markdown):
                query_list.append(markdown)
        print(f"The length of query_list is: {len(query_list)}")
        client = OpenAI(
            api_key=self.dash_scope_key,  # ç›´æ¥é…ç½®api keyä»¥è®¿é—®é˜¿é‡Œäº‘æä¾›çš„æœåŠ¡
            base_url=self.bailian_endpoint  # ç™¾ç‚¼æœåŠ¡çš„base_url
        )

        # é˜¿é‡Œäº‘çš„è¯åµŒå…¥æ¥å£ä¸€æ¬¡æ€§æœ€å¤šæ¥å—10ä¸ªå­—ç¬¦ä¸²è¾“å…¥ï¼Œæ‰€ä»¥è¦åˆ†å¼€æ¥ä¸Šä¼ 
        vector_list = []
        for i in range(0,len(query_list)-10,10):
            completion = client.embeddings.create(
                model="text-embedding-v4",
                input=query_list[i:i+10], # ä¸€æ¬¡æ€§æ‰¹ä¸Šä¼ 10ä¸ªæ–‡æœ¬å—è‡³é˜¿é‡Œäº‘ç™¾ç‚¼API
                dimensions=1024,  # æŒ‡å®šå‘é‡ç»´åº¦ï¼ˆä»… text-embedding-v3åŠ text-embedding-v4æ”¯æŒè¯¥å‚æ•°ï¼‰
                # ç›®å‰é¡¹ç›®å¯ç”¨çš„DashVectorä»…æ”¯æŒå›ºå®šé•¿åº¦çš„å‘é‡ï¼Œæ ¹æ®ç®€å•è°ƒæŸ¥ï¼Œé€‰æ‹©1024ç»´è¿™ä¸ªæ¯”è¾ƒé€šç”¨çš„é€‰æ‹©
                encoding_format="float"
            )
            vector_list.append(completion.data)
        flatten_vector_list = [
            np.array(item.embedding,dtype=float)
            for sublist in vector_list
            for item in sublist
        ]
        rawcontent_vector_pair = list(zip(query_list, flatten_vector_list))
        return rawcontent_vector_pair
        # print(completion.model_dump_json())
        # return vector_list, query_list # åŒæ—¶è¿”å›åŸæ–‡å†…å®¹å’Œå…¶å¯¹åº”çš„å‘é‡
    def file2VectorDB(self, file_path: str, use_content_filter: bool = True, is_oss_key: bool = False, update_index: bool = True) -> bool:
        """
        å°†æ–‡æ¡£è§£æå¹¶ä¸Šä¼ è‡³å‘é‡æ•°æ®åº“

        Args:
            file_path (str): æ–‡æ¡£è·¯å¾„æˆ–OSSé”®å
            use_content_filter (bool): æ˜¯å¦ä½¿ç”¨å†…å®¹è¿‡æ»¤åŠŸèƒ½ï¼Œé»˜è®¤ä¸ºTrue
            is_oss_key (bool): æ˜¯å¦ä¸ºOSSé”®å
            update_index (bool): æ˜¯å¦æ›´æ–°æ–‡ä»¶ç´¢å¼•çš„å‘é‡åŒ–çŠ¶æ€

        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            filename = os.path.basename(file_path) if not is_oss_key else os.path.basename(file_path)

            if use_content_filter:
                # ä½¿ç”¨æ–°çš„å†…å®¹è¿‡æ»¤æ–¹æ³•
                filtered_segments = self.content_filter(file_path, is_oss_key=is_oss_key)
                if filtered_segments:
                    print(f"âœ… ä½¿ç”¨å†…å®¹è¿‡æ»¤æ–¹æ³•æˆåŠŸå¤„ç†æ–‡æ¡£ï¼Œæå–äº† {len(filtered_segments)} ä¸ªé«˜å¯†åº¦ä¿¡æ¯æ®µè½")

                    # æ›´æ–°å‘é‡åŒ–çŠ¶æ€
                    if update_index and is_oss_key:
                        try:
                            file_manager = FileIndexManager()
                            file_manager.update_vectorization_status(filename, True)
                        except Exception as e:
                            print(f"âš ï¸  æ›´æ–°å‘é‡åŒ–çŠ¶æ€å¤±è´¥: {e}")

                    return True
                else:
                    print("âš ï¸ å†…å®¹è¿‡æ»¤æœªæå–åˆ°æœ‰æ•ˆä¿¡æ¯ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•")
                    # å¦‚æœå†…å®¹è¿‡æ»¤å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
                    use_content_filter = False

            if not use_content_filter:
                # ä½¿ç”¨åŸå§‹æ–¹æ³•
                actual_file_path = file_path
                temp_file_path = None

                try:
                    # å¦‚æœæ˜¯OSSé”®åï¼Œå…ˆä¸‹è½½åˆ°æœ¬åœ°
                    if is_oss_key:
                        temp_file_path = self.download_file_from_oss(file_path)
                        if not temp_file_path:
                            print(f"âŒ æ— æ³•ä»OSSä¸‹è½½æ–‡ä»¶: {file_path}")
                            return False
                        actual_file_path = temp_file_path

                    response = self.__doc_analysation(actual_file_path)
                    vector_pairs = self.__content_vectorised(response)
                    client = dashvector.Client(
                        api_key=self.dash_vector_key,
                        endpoint=self.vectorDB_endpoint
                    )
                    collection = client.get(name='simple_collection')
                    for i in range(len(vector_pairs)):
                        ret = collection.insert(
                            Doc(
                                id=str(uuid.uuid4()),
                                vector=vector_pairs[i][1],
                                fields={
                                    "raw_text": vector_pairs[i][0],
                                    "sub_title": "original_content"
                                }
                            )
                        )
                        assert ret  # åˆ¤æ–­æ’å…¥æ“ä½œæ˜¯å¦æˆåŠŸ
                    print(f"âœ… ä½¿ç”¨åŸå§‹æ–¹æ³•æˆåŠŸå¤„ç†æ–‡æ¡£ï¼Œå­˜å‚¨äº† {len(vector_pairs)} ä¸ªæ–‡æœ¬æ®µè½")

                    # æ›´æ–°å‘é‡åŒ–çŠ¶æ€
                    if update_index and is_oss_key:
                        try:
                            file_manager = FileIndexManager()
                            file_manager.update_vectorization_status(filename, True)
                        except Exception as e:
                            print(f"âš ï¸  æ›´æ–°å‘é‡åŒ–çŠ¶æ€å¤±è´¥: {e}")

                finally:
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if temp_file_path and os.path.exists(temp_file_path):
                        try:
                            os.unlink(temp_file_path)
                        except Exception as e:
                            print(f"âš ï¸  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

            return True

        except Exception as e:
            print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            return False

    def batch_process_recent_files(self, days: int = 30, use_content_filter: bool = True) -> dict:
        """
        æ‰¹é‡å¤„ç†OSSä¸­æœ€è¿‘ä¸Šä¼ çš„æ–‡ä»¶

        Args:
            days (int): å¤„ç†æœ€è¿‘å‡ å¤©çš„æ–‡ä»¶ï¼Œé»˜è®¤30å¤©
            use_content_filter (bool): æ˜¯å¦ä½¿ç”¨å†…å®¹è¿‡æ»¤åŠŸèƒ½

        Returns:
            dict: å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            # è·å–æœ€è¿‘ä¸Šä¼ çš„æ–‡ä»¶
            recent_files = self.get_recent_files_from_oss(days)

            if not recent_files:
                print(f"âš ï¸  æœªæ‰¾åˆ°æœ€è¿‘ {days} å¤©å†…ä¸Šä¼ çš„æ–‡ä»¶")
                return {"total": 0, "processed": 0, "failed": 0, "skipped": 0}

            # è¿‡æ»¤å‡ºæœªå‘é‡åŒ–çš„æ–‡ä»¶
            unvectorized_files = [f for f in recent_files if not f.get("vectorized", False)]

            print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"  - æœ€è¿‘ {days} å¤©å†…ä¸Šä¼ çš„æ–‡ä»¶: {len(recent_files)}")
            print(f"  - æœªå‘é‡åŒ–çš„æ–‡ä»¶: {len(unvectorized_files)}")

            if not unvectorized_files:
                print("âœ… æ‰€æœ‰æœ€è¿‘ä¸Šä¼ çš„æ–‡ä»¶éƒ½å·²å‘é‡åŒ–")
                return {"total": len(recent_files), "processed": 0, "failed": 0, "skipped": len(recent_files)}

            # æ‰¹é‡å¤„ç†æ–‡ä»¶
            results = {"total": len(unvectorized_files), "processed": 0, "failed": 0, "skipped": 0}

            for i, file_record in enumerate(unvectorized_files, 1):
                filename = file_record["filename"]
                oss_key = file_record["oss_key"]

                print(f"\n[{i}/{len(unvectorized_files)}] å¤„ç†æ–‡ä»¶: {filename}")

                try:
                    success = self.file2VectorDB(
                        oss_key,
                        use_content_filter=use_content_filter,
                        is_oss_key=True,
                        update_index=True
                    )

                    if success:
                        results["processed"] += 1
                        print(f"âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ: {filename}")
                    else:
                        results["failed"] += 1
                        print(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {filename}")

                except Exception as e:
                    results["failed"] += 1
                    print(f"âŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸: {filename}, é”™è¯¯: {e}")

            print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
            print(f"ğŸ“Š å¤„ç†ç»“æœ:")
            print(f"  - æ€»æ–‡ä»¶æ•°: {results['total']}")
            print(f"  - å¤„ç†æˆåŠŸ: {results['processed']}")
            print(f"  - å¤„ç†å¤±è´¥: {results['failed']}")
            print(f"  - è·³è¿‡æ–‡ä»¶: {results['skipped']}")

            return results

        except Exception as e:
            print(f"âŒ æ‰¹é‡å¤„ç†å¤±è´¥: {e}")
            return {"total": 0, "processed": 0, "failed": 0, "skipped": 0}

class UserRetrieve:
    def __init__(self):
        """
        ä».envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­åŠ è½½APIå¯†é’¥ï¼Œè¿™ä¸ªç±»éœ€è¦è¯»å–çš„api keyæœ‰ï¼š
        DASHSCOPE_API_KEY=SECRET
        DASH_VECTOR_API=SECRET

        ä¼˜å…ˆä».envæ–‡ä»¶åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½
        """
        print("ğŸ”„ UserRetrieveç±»åˆå§‹åŒ–ï¼šå¼€å§‹åŠ è½½APIå¯†é’¥...")

        # åŠ è½½APIå¯†é’¥
        keys, success = load_api_keys()

        # UserRetrieveåªéœ€è¦éƒ¨åˆ†å¯†é’¥
        required_keys = ["DASH_VECTOR_API"]
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„APIå¯†é’¥ï¼ˆDASHSCOPE_API_KEY æˆ– ALIBABA_QWEN_API_KEYï¼‰
        has_llm_key = keys.get("DASHSCOPE_API_KEY") or keys.get("ALIBABA_QWEN_API_KEY")

        missing_keys = [key for key in required_keys if not keys.get(key)]
        if missing_keys or not has_llm_key:
            missing_keys.append("DASHSCOPE_API_KEY æˆ– ALIBABA_QWEN_API_KEY")
            raise ValueError(f"âŒ UserRetrieveç±»ç¼ºå°‘å¿…éœ€çš„APIå¯†é’¥: {missing_keys}")

        # è®¾ç½®å®ä¾‹å˜é‡
        self.dash_scope_key = keys["DASHSCOPE_API_KEY"]
        self.qwen_api_key = keys["ALIBABA_QWEN_API_KEY"] or keys["DASHSCOPE_API_KEY"]  # ä¼˜å…ˆä½¿ç”¨QWENå¯†é’¥
        self.dash_vector_key = keys["DASH_VECTOR_API"]

        # è®¿é—®çš„åŸŸå
        # é˜¿é‡Œäº‘çš„å‘é‡æ•°æ®åº“æœåŠ¡çš„èŠ‚ç‚¹åœ°å€
        self.vectorDB_endpoint = "vrs-cn-6sa4axaiv0001c.dashvector.cn-shanghai.aliyuncs.com"
        self.bailian_endpoint = "https://dashscope.aliyuncs.com/compatible-mode/v1"

        print("âœ… UserRetrieveç±»åˆå§‹åŒ–å®Œæˆ")
    def __user_query_vectorised(self, raw_user_query: str):
        """
        å¢å¼ºçš„ç”¨æˆ·æŸ¥è¯¢å‘é‡åŒ–æ–¹æ³•ï¼Œæ”¯æŒä¸­è‹±æ–‡æ£€æµ‹å’Œç¿»è¯‘

        Args:
            raw_user_query (str): ç”¨æˆ·åŸå§‹æŸ¥è¯¢

        Returns:
            dict: åŒ…å«åŸæ–‡å’Œç¿»è¯‘å‰¯æœ¬çš„å‘é‡åŒ–ç»“æœ
                {
                    'original': {'text': str, 'vector': np.array, 'language': str},
                    'translated': {'text': str, 'vector': np.array, 'language': str}
                }
        """
        # 1. ä½¿ç”¨æ–°çš„ç¿»è¯‘æœåŠ¡è¿›è¡Œè¯­è¨€æ£€æµ‹å’Œç¿»è¯‘
        try:
            from ..services.translation_service import get_translation_service
            translation_service = get_translation_service()
            translation_result = translation_service.query_translation(raw_user_query)

            # æå–ç¿»è¯‘ç»“æœ
            original_text = translation_result['original']['text']
            original_language = translation_result['original']['language']
            translated_text = translation_result['translated']['text']
            translated_language = translation_result['translated']['language']

            print(f"ğŸ” æ£€æµ‹åˆ°æŸ¥è¯¢è¯­è¨€: {original_language}")
            print(f"ğŸ“ åŸæ–‡: {original_text}")
            print(f"ğŸ”„ ç¿»è¯‘: {translated_text}")

        except Exception as e:
            print(f"âš ï¸ æ–°ç¿»è¯‘æœåŠ¡å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰ç¿»è¯‘æ–¹æ³•: {e}")
            # å›é€€åˆ°åŸæœ‰çš„ç¿»è¯‘æ–¹æ³•
            detected_language = detect_language(raw_user_query)
            print(f"ğŸ” æ£€æµ‹åˆ°æŸ¥è¯¢è¯­è¨€: {detected_language}")

            if detected_language == 'chinese':
                translated_query = translate_text(
                    raw_user_query,
                    'english',
                    self.qwen_api_key,
                    self.bailian_endpoint
                )
                translated_language = 'english'
            else:
                translated_query = translate_text(
                    raw_user_query,
                    'chinese',
                    self.qwen_api_key,
                    self.bailian_endpoint
                )
                translated_language = 'chinese'

            # ç»Ÿä¸€å˜é‡å
            original_text = raw_user_query
            original_language = detected_language
            translated_text = translated_query

        # 3. å¯¹åŸæ–‡å’Œç¿»è¯‘å‰¯æœ¬è¿›è¡Œå‘é‡åŒ–
        client = OpenAI(
            api_key=self.dash_scope_key,
            base_url=self.bailian_endpoint
        )

        # æ‰¹é‡å‘é‡åŒ–åŸæ–‡å’Œç¿»è¯‘
        completion = client.embeddings.create(
            model="text-embedding-v4",
            input=[original_text, translated_text],
            dimensions=1024,
            encoding_format="float"
        )

        # 4. æ„å»ºè¿”å›ç»“æœ
        result = {
            'original': {
                'text': original_text,
                'vector': np.array(completion.data[0].embedding, dtype=float),
                'language': original_language
            },
            'translated': {
                'text': translated_text,
                'vector': np.array(completion.data[1].embedding, dtype=float),
                'language': translated_language
            }
        }

        return result
    def retrieve_topK(self, user_query: str, k: int):
        """
        å¢å¼ºçš„TopKæ£€ç´¢æ–¹æ³•ï¼Œæ”¯æŒä¸­è‹±æ–‡åŒè¯­æ£€ç´¢

        Args:
            user_query (str): ç”¨æˆ·æŸ¥è¯¢
            k (int): è¿”å›ç»“æœæ•°é‡

        Returns:
            list: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ éƒ½æ˜¯å­—ç¬¦ä¸²
        """
        # 1. è·å–ç”¨æˆ·æŸ¥è¯¢çš„å‘é‡åŒ–ç»“æœï¼ˆåŒ…å«åŸæ–‡å’Œç¿»è¯‘ï¼‰
        vectorised_queries = self.__user_query_vectorised(user_query)

        # 2. è¿æ¥å‘é‡æ•°æ®åº“
        client = dashvector.Client(
            api_key=self.dash_vector_key,
            endpoint=self.vectorDB_endpoint
        )
        collection = client.get(name='simple_collection')

        # 3. è®¡ç®—æ¯æ¬¡æ£€ç´¢çš„æ•°é‡ï¼ˆå‘ä¸Šå–æ•´ï¼‰
        k_per_query = math.ceil(k / 2)
        print(f"ğŸ” æ¯ä¸ªæŸ¥è¯¢æ£€ç´¢ {k_per_query} ä¸ªç»“æœï¼Œæ€»å…±æ£€ç´¢ {k} ä¸ªç»“æœ")

        # 4. ä½¿ç”¨åŸæ–‡è¿›è¡Œæ£€ç´¢
        print(f"ğŸ” ä½¿ç”¨åŸæ–‡æ£€ç´¢: {vectorised_queries['original']['text']}")
        ret_original = collection.query(
            vector=vectorised_queries['original']['vector'],
            topk=k_per_query,
            output_fields=['raw_text', 'sub_title'],
            include_vector=True
        )

        # 5. ä½¿ç”¨ç¿»è¯‘å‰¯æœ¬è¿›è¡Œæ£€ç´¢
        print(f"ğŸ” ä½¿ç”¨ç¿»è¯‘æ£€ç´¢: {vectorised_queries['translated']['text']}")
        ret_translated = collection.query(
            vector=vectorised_queries['translated']['vector'],
            topk=k_per_query,
            output_fields=['raw_text', 'sub_title'],
            include_vector=True
        )

        # 6. åˆå¹¶æ£€ç´¢ç»“æœ
        retrieve_result = []
        seen_texts = set()  # ç”¨äºå»é‡

        # æ·»åŠ åŸæ–‡æ£€ç´¢ç»“æœ
        if ret_original and ret_original.output:
            print(f"âœ… åŸæ–‡æ£€ç´¢æˆåŠŸï¼Œè·å¾— {len(ret_original.output)} ä¸ªç»“æœ")
            for content in ret_original.output:
                text = content.fields["raw_text"]
                if text not in seen_texts:
                    retrieve_result.append(text)
                    seen_texts.add(text)
        else:
            print("âš ï¸ åŸæ–‡æ£€ç´¢å¤±è´¥æˆ–æ— ç»“æœ")

        # æ·»åŠ ç¿»è¯‘æ£€ç´¢ç»“æœ
        if ret_translated and ret_translated.output:
            print(f"âœ… ç¿»è¯‘æ£€ç´¢æˆåŠŸï¼Œè·å¾— {len(ret_translated.output)} ä¸ªç»“æœ")
            for content in ret_translated.output:
                text = content.fields["raw_text"]
                if text not in seen_texts:
                    retrieve_result.append(text)
                    seen_texts.add(text)
        else:
            print("âš ï¸ ç¿»è¯‘æ£€ç´¢å¤±è´¥æˆ–æ— ç»“æœ")

        print(f"ğŸ‰ æ€»å…±æ£€ç´¢åˆ° {len(retrieve_result)} ä¸ªå”¯ä¸€ç»“æœ")
        return retrieve_result

if __name__ == "__main__":
    # test_user = UserRetrieve()
    # retrieve_list = test_user.retrieve_topK("æ¯æ—¥è¥å…»å»ºè®®", 3)
    # print(len(retrieve_list))
    # print(retrieve_list)

    # ä½¿ç”¨è·¨å¹³å°è·¯å¾„å¤„ç†
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sample_doc_path = os.path.join(current_dir, "testMaterials", "ä¸­å›½æˆå¹´äººè‚‰ç±»é£Ÿç‰©æ‘„å…¥ä¸ä»£è°¢ç»¼åˆå¾çš„ç›¸å…³æ€§ç ”ç©¶.pdf")

    obj = Document()
    result = obj.file2VectorDB(sample_doc_path)
    print(result)

