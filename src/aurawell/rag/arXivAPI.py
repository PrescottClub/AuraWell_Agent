import urllib.request
import urllib.parse
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import os
import re
import time
import ssl
import tempfile
from rag_utils import get_download_path
from oss_utils import OSSManager
from file_index_manager import FileIndexManager

def fetch_papers_by_keyword(keyword, max_results=10, days_ago=None):
    """
    æ ¹æ®å…³é”®è¯ä»arXivæ£€ç´¢ç›¸å…³è®ºæ–‡

    å‚æ•°:
    keyword (str): æœç´¢å…³é”®è¯
    max_results (int): è¿”å›çš„æœ€å¤§è®ºæ–‡æ•°é‡ (é»˜è®¤10)
    days_ago (int): ä»…æ£€ç´¢æœ€è¿‘Nå¤©çš„è®ºæ–‡ (å¯é€‰)

    è¿”å›:
    list: åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    # æ„å»ºæŸ¥è¯¢å‚æ•° - å¤„ç†å¤šè¯å…³é”®è¯
    # å¦‚æœå…³é”®è¯åŒ…å«ç©ºæ ¼ï¼Œç”¨å¼•å·åŒ…å›´æˆ–ç”¨ANDè¿æ¥
    if ' ' in keyword:
        search_term = f'all:"{keyword}"'  # ç”¨å¼•å·åŒ…å›´å¤šè¯å…³é”®è¯
    else:
        search_term = f"all:{keyword}"

    base_params = {
        "search_query": search_term,       # ä½¿ç”¨å¤„ç†åçš„å…³é”®è¯
        "sortBy": "submittedDate",         # æŒ‰æäº¤æ—¥æœŸæ’åº
        "sortOrder": "descending"          # æœ€æ–°ä¼˜å…ˆ
    }

    # æ·»åŠ æ—¶é—´èŒƒå›´è¿‡æ»¤
    if days_ago:
        date_str = (datetime.now() - timedelta(days=days_ago)).strftime("%Y-%m-%d")
        base_params["search_query"] += f" AND submittedDate:[{date_str} TO NOW]"

    # æ·»åŠ ç»“æœé™åˆ¶
    query_params = {**base_params, "start": 0, "max_results": max_results}

    try:
        # æ„å»ºè¯·æ±‚URL
        api_url = "http://export.arxiv.org/api/query?" + urllib.parse.urlencode(query_params)

        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œå¿½ç•¥è¯ä¹¦éªŒè¯ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        # å‘é€APIè¯·æ±‚
        request = urllib.request.Request(api_url)
        with urllib.request.urlopen(request, timeout=30, context=ssl_context) as response:
            xml_data = response.read().decode('utf-8')

        # è§£æXMLç»“æœ
        return parse_arxiv_xml(xml_data)

    except Exception as e:
        print(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
        return []

def parse_arxiv_xml(xml_string):
    """
    è§£æarXivè¿”å›çš„XMLæ•°æ®
    
    è¿”å›åŒ…å«è®ºæ–‡ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
    """
    root = ET.fromstring(xml_string)
    namespace = {'atom': 'http://www.w3.org/2005/Atom'}
    papers = []
    
    for entry in root.findall('atom:entry', namespace):
        paper = {
            "title": entry.find('atom:title', namespace).text.strip(),
            "id": entry.find('atom:id', namespace).text.split('/')[-1],
            "published": entry.find('atom:published', namespace).text,
            "summary": entry.find('atom:summary', namespace).text.strip(),
            "authors": [a.find('atom:name', namespace).text 
                        for a in entry.findall('atom:author', namespace)],
            "pdf_url": None
        }
        
        # æå–PDFé“¾æ¥ - ä¿®å¤é“¾æ¥æå–é€»è¾‘
        for link in entry.findall('atom:link', namespace):
            if link.get('title') == 'pdf':
                paper['pdf_url'] = link.get('href')
                break

        # å¦‚æœæ²¡æ‰¾åˆ°PDFé“¾æ¥ï¼Œå°è¯•ä»IDæ„å»ºPDF URL
        if not paper['pdf_url'] and paper['id']:
            paper['pdf_url'] = f"https://arxiv.org/pdf/{paper['id']}.pdf"
        
        papers.append(paper)
    
    return papers
def download_pdf_to_oss(pdf_url, filename=None):
    """
    ä¸‹è½½è®ºæ–‡PDFå¹¶ç›´æ¥ä¸Šä¼ åˆ°OSSäº‘å­˜å‚¨

    å‚æ•°:
    pdf_url (str): PDFæ–‡ä»¶çš„URL
    filename (str): è‡ªå®šä¹‰æ–‡ä»¶åï¼ˆå¯é€‰ï¼‰

    è¿”å›:
    tuple: (success: bool, oss_key: str, filename: str)
    """
    if not pdf_url:
        print("âŒ é”™è¯¯ï¼šæœªæä¾›æœ‰æ•ˆçš„PDF URL")
        return False, None, None

    try:
        # åˆå§‹åŒ–OSSç®¡ç†å™¨å’Œæ–‡ä»¶ç´¢å¼•ç®¡ç†å™¨
        oss_manager = OSSManager()
        file_index_manager = FileIndexManager()

        # ä»URLæå–æ–‡ä»¶å
        if not filename:
            filename_match = re.search(r'/([^/]+\.pdf)$', pdf_url)
            if not filename_match:
                filename = f"paper_{int(time.time())}.pdf"
            else:
                filename = filename_match.group(1)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨äºç´¢å¼•ä¸­
        if file_index_manager.file_exists_in_index(filename):
            print(f"âš ï¸  æ–‡ä»¶å·²å­˜åœ¨äºOSSä¸­ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
            return True, f"nutrition/{filename}", filename

        # æ„å»ºOSSé”®å
        oss_key = f"nutrition/{filename}"

        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œå¿½ç•¥è¯ä¹¦éªŒè¯ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        # åˆ›å»ºopener with SSL context
        opener = urllib.request.build_opener(urllib.request.HTTPSHandler(context=ssl_context))
        urllib.request.install_opener(opener)

        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¸‹è½½
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            temp_path = temp_file.name

            try:
                # ä¸‹è½½æ–‡ä»¶åˆ°ä¸´æ—¶ä½ç½®
                print(f"ğŸ”„ æ­£åœ¨ä¸‹è½½: {filename}")
                urllib.request.urlretrieve(pdf_url, temp_path)

                # ä¸Šä¼ åˆ°OSS
                print(f"ğŸ”„ æ­£åœ¨ä¸Šä¼ åˆ°OSS: {oss_key}")
                upload_success = oss_manager.upload_file(temp_path, oss_key)

                if upload_success:
                    # æ·»åŠ åˆ°æ–‡ä»¶ç´¢å¼•
                    index_success = file_index_manager.add_file_record(
                        filename=filename,
                        oss_key=oss_key,
                        vectorized=False
                    )

                    if index_success:
                        print(f"âœ… æ–‡ä»¶æˆåŠŸä¸‹è½½å¹¶ä¸Šä¼ åˆ°OSS: {filename}")
                        return True, oss_key, filename
                    else:
                        print(f"âš ï¸  æ–‡ä»¶ä¸Šä¼ æˆåŠŸä½†ç´¢å¼•æ›´æ–°å¤±è´¥: {filename}")
                        return True, oss_key, filename
                else:
                    print(f"âŒ æ–‡ä»¶ä¸Šä¼ åˆ°OSSå¤±è´¥: {filename}")
                    return False, None, filename

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_path)
                except:
                    pass

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {str(e)}")
        return False, None, filename

def download_pdf(pdf_url, save_dir=None):
    """
    ä¸‹è½½è®ºæ–‡PDFåˆ°æŒ‡å®šç›®å½•ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ç”¨äºå…¼å®¹æ€§ï¼‰

    å‚æ•°:
    pdf_url (str): PDFæ–‡ä»¶çš„URL
    save_dir (str): ä¿å­˜ç›®å½•ï¼ˆå¯é€‰ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨è·¨å¹³å°è·¯å¾„

    è¿”å›:
    str: ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
    """
    if not pdf_url:
        print("é”™è¯¯ï¼šæœªæä¾›æœ‰æ•ˆçš„PDF URL")
        return None

    # ä½¿ç”¨é»˜è®¤è·¯å¾„æˆ–è‡ªå®šä¹‰è·¯å¾„
    if save_dir is None:
        save_dir = get_download_path()

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)

    # ä»URLæå–æ–‡ä»¶å
    filename = re.search(r'/([^/]+\.pdf)$', pdf_url)
    if not filename:
        filename = f"paper_{int(time.time())}.pdf"
    else:
        filename = filename.group(1)

    file_path = os.path.join(save_dir, filename)

    try:
        # åˆ›å»ºSSLä¸Šä¸‹æ–‡ï¼Œå¿½ç•¥è¯ä¹¦éªŒè¯ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE

        # åˆ›å»ºopener with SSL context
        opener = urllib.request.build_opener(urllib.request.HTTPSHandler(context=ssl_context))
        urllib.request.install_opener(opener)

        # ä¸‹è½½æ–‡ä»¶
        urllib.request.urlretrieve(pdf_url, file_path)
        print(f"å·²ä¸‹è½½: {file_path}")
        return file_path
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥: {str(e)}")
        return None

def generate_ris(paper, save_dir="exports"):
    """
    ç”ŸæˆRISæ ¼å¼æ–‡çŒ®å¼•ç”¨æ–‡ä»¶
    
    å‚æ•°:
    paper (dict): è®ºæ–‡ä¿¡æ¯å­—å…¸
    save_dir (str): ä¿å­˜ç›®å½•
    
    è¿”å›:
    str: RISæ–‡ä»¶è·¯å¾„
    """
    os.makedirs(save_dir, exist_ok=True)
    filename = f"{paper['id']}.ris"
    file_path = os.path.join(save_dir, filename)
    
    # RISæ ¼å¼æ¨¡æ¿
    ris_content = f"""TY  - JOUR
TI  - {paper['title']}
AU  - {paper['authors'][0]}  # åªä½¿ç”¨ç¬¬ä¸€ä½œè€…
PY  - {paper['published'][:4]}  # ä»…å¹´ä»½
AB  - {paper['summary'][:500]}  # æ‘˜è¦æˆªæ–­
UR  - https://arxiv.org/abs/{paper['id']}
DO  - 10.48550/arXiv.{paper['id']}
ER  - -"""
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(ris_content)
    
    print(f"å·²ç”ŸæˆRISå¼•ç”¨æ–‡ä»¶: {file_path}")
    return file_path
def export_papers_by_keyword_to_oss(keyword, k=10):
    """
    æ ¹æ®å…³é”®è¯æœç´¢å¹¶å¯¼å‡ºkä¸ªç›¸å…³æ–‡çŒ®åˆ°OSSäº‘å­˜å‚¨

    å‚æ•°:
    keyword (str): æœç´¢å…³é”®è¯
    k (int): å¯¼å‡ºçš„æ–‡çŒ®æ•°é‡ (é»˜è®¤10)

    è¿”å›:
    list: æˆåŠŸä¸‹è½½çš„æ–‡çŒ®ä¿¡æ¯åˆ—è¡¨
    """
    print(f"ğŸ” æ­£åœ¨æœç´¢å…³é”®è¯ '{keyword}' ç›¸å…³çš„è®ºæ–‡...")

    # æ£€ç´¢è®ºæ–‡
    results = fetch_papers_by_keyword(keyword, max_results=k)

    if not results:
        print(f"âŒ æœªæ‰¾åˆ°ä¸å…³é”®è¯ '{keyword}' ç›¸å…³çš„è®ºæ–‡")
        return []

    print(f"âœ… æ‰¾åˆ° {len(results)} ç¯‡ä¸ '{keyword}' ç›¸å…³çš„è®ºæ–‡")

    downloaded_papers = []

    # å¤„ç†æ¯ç¯‡è®ºæ–‡
    for i, paper in enumerate(results, 1):
        print(f"\n[{i}/{len(results)}] å¤„ç†è®ºæ–‡: {paper['title'][:80]}...")

        # ä¸‹è½½PDFåˆ°OSS
        if paper['pdf_url']:
            success, oss_key, filename = download_pdf_to_oss(paper['pdf_url'])
            if success:
                downloaded_papers.append({
                    'title': paper['title'],
                    'authors': paper['authors'],
                    'published': paper['published'],
                    'oss_key': oss_key,
                    'filename': filename,
                    'arxiv_id': paper['id']
                })
            else:
                print(f"  âŒ ä¸‹è½½å¤±è´¥: {paper['title'][:50]}...")
        else:
            print("  âš ï¸  æœªæ‰¾åˆ°PDFé“¾æ¥ï¼Œè·³è¿‡ä¸‹è½½")

    print(f"\nğŸ‰ å¯¼å‡ºå®Œæˆï¼æˆåŠŸä¸‹è½½ {len(downloaded_papers)} ç¯‡è®ºæ–‡åˆ°OSSäº‘å­˜å‚¨")

    # æ˜¾ç¤ºä¸‹è½½çš„è®ºæ–‡åˆ—è¡¨
    if downloaded_papers:
        print("\nğŸ“š å·²ä¸‹è½½çš„è®ºæ–‡:")
        for paper in downloaded_papers:
            print(f"  - {paper['title'][:60]}... ({paper['arxiv_id']})")

    return downloaded_papers

def export_papers_by_keyword(keyword, k=10, save_dir=None):
    """
    æ ¹æ®å…³é”®è¯æœç´¢å¹¶å¯¼å‡ºkä¸ªç›¸å…³æ–‡çŒ®åˆ°æŒ‡å®šç›®å½•ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ç”¨äºå…¼å®¹æ€§ï¼‰

    å‚æ•°:
    keyword (str): æœç´¢å…³é”®è¯
    k (int): å¯¼å‡ºçš„æ–‡çŒ®æ•°é‡ (é»˜è®¤10)
    save_dir (str): ä¿å­˜ç›®å½•ï¼Œé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ nutrition_article æ–‡ä»¶å¤¹

    è¿”å›:
    list: æˆåŠŸä¸‹è½½çš„æ–‡çŒ®ä¿¡æ¯åˆ—è¡¨
    """
    # è®¾ç½®ä¿å­˜ç›®å½•
    if save_dir is None:
        save_dir = get_download_path()

    print(f"æ­£åœ¨æœç´¢å…³é”®è¯ '{keyword}' ç›¸å…³çš„è®ºæ–‡...")

    # æ£€ç´¢è®ºæ–‡
    results = fetch_papers_by_keyword(keyword, max_results=k)

    if not results:
        print(f"æœªæ‰¾åˆ°ä¸å…³é”®è¯ '{keyword}' ç›¸å…³çš„è®ºæ–‡")
        return []

    print(f"\næ‰¾åˆ° {len(results)} ç¯‡ä¸ '{keyword}' ç›¸å…³çš„è®ºæ–‡:")
    print(f"ä¿å­˜ç›®å½•: {save_dir}")

    downloaded_papers = []

    # å¤„ç†æ¯ç¯‡è®ºæ–‡
    for i, paper in enumerate(results, 1):
        print(f"\n[{i}/{len(results)}] å¤„ç†è®ºæ–‡: {paper['title'][:80]}...")

        # ä¸‹è½½PDF
        if paper['pdf_url']:
            pdf_path = download_pdf(paper['pdf_url'], save_dir=save_dir)
            if pdf_path:
                downloaded_papers.append({
                    'title': paper['title'],
                    'authors': paper['authors'],
                    'published': paper['published'],
                    'pdf_path': pdf_path,
                    'arxiv_id': paper['id']
                })
        else:
            print("  æœªæ‰¾åˆ°PDFé“¾æ¥ï¼Œè·³è¿‡ä¸‹è½½")

    print(f"\nå¯¼å‡ºå®Œæˆï¼æˆåŠŸä¸‹è½½ {len(downloaded_papers)} ç¯‡è®ºæ–‡åˆ° {save_dir}")

    # æ˜¾ç¤ºä¸‹è½½çš„è®ºæ–‡åˆ—è¡¨
    if downloaded_papers:
        print("\nå·²ä¸‹è½½çš„è®ºæ–‡:")
        for paper in downloaded_papers:
            print(f"  - {paper['title'][:60]}... ({paper['arxiv_id']})")

    return downloaded_papers

def main_workflow():
    """ä¸»å·¥ä½œæµç¨‹ç¤ºä¾‹ï¼šä½¿ç”¨é»˜è®¤å…³é”®è¯æœç´¢è¥å…»å­¦è®ºæ–‡å¹¶ä¸Šä¼ åˆ°OSS"""
    print("ğŸš€ å¼€å§‹è¥å…»å­¦è®ºæ–‡æ‰¹é‡å¯¼å…¥å·¥ä½œæµç¨‹")

    # ä½¿ç”¨æ–°çš„OSSå­˜å‚¨åŠŸèƒ½
    papers = export_papers_by_keyword_to_oss("nutrition", k=5)

    if papers:
        print(f"\nâœ… æˆåŠŸå¯¼å…¥ {len(papers)} ç¯‡è®ºæ–‡åˆ°OSSäº‘å­˜å‚¨")

        # æ˜¾ç¤ºæ–‡ä»¶ç´¢å¼•çŠ¶æ€
        try:
            from file_index_manager import FileIndexManager
            file_manager = FileIndexManager()
            all_files = file_manager.get_all_files()
            unvectorized = file_manager.get_unvectorized_files()

            print(f"ğŸ“Š æ–‡ä»¶ç´¢å¼•ç»Ÿè®¡:")
            print(f"  - æ€»æ–‡ä»¶æ•°: {len(all_files)}")
            print(f"  - æœªå‘é‡åŒ–æ–‡ä»¶æ•°: {len(unvectorized)}")

        except Exception as e:
            print(f"âš ï¸  è·å–æ–‡ä»¶ç´¢å¼•ç»Ÿè®¡å¤±è´¥: {e}")
    else:
        print("âŒ æœªæˆåŠŸå¯¼å…¥ä»»ä½•è®ºæ–‡")

def main_workflow_local():
    """ä¸»å·¥ä½œæµç¨‹ç¤ºä¾‹ï¼šä½¿ç”¨é»˜è®¤å…³é”®è¯æœç´¢è¥å…»å­¦è®ºæ–‡ï¼ˆæœ¬åœ°å­˜å‚¨ï¼‰"""
    export_papers_by_keyword("nutrition", k=5)

if __name__ == "__main__":
    main_workflow()
