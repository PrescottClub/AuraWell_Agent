"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
éªŒè¯é‡æ„åä»£ç çš„æ€§èƒ½æ”¶ç›Šå’Œç³»ç»Ÿç¨³å®šæ€§
"""

import asyncio
import time
import statistics
import json
from typing import List, Dict, Any
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "src"))

from aurawell.langchain_agent.mcp_tools_manager import MCPToolsManager
from aurawell.monitoring.performance_monitor_refactored import PerformanceMonitorRefactored


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.results = {}
        self.mcp_manager = MCPToolsManager()
        self.performance_monitor = PerformanceMonitorRefactored(collection_interval=1)
    
    async def benchmark_mcp_tools_execution(self, iterations: int = 100) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•MCPå·¥å…·æ‰§è¡Œæ€§èƒ½"""
        print(f"ğŸ”§ å¼€å§‹MCPå·¥å…·æ‰§è¡Œæ€§èƒ½æµ‹è¯• ({iterations}æ¬¡è¿­ä»£)...")
        
        # æµ‹è¯•calculatorå·¥å…·
        calculator_times = []
        for i in range(iterations):
            start_time = time.perf_counter()
            
            result = await self.mcp_manager._call_calculator("bmi", {
                "weight": 70 + (i % 30),  # å˜åŒ–çš„ä½“é‡
                "height": 175 + (i % 20)  # å˜åŒ–çš„èº«é«˜
            })
            
            end_time = time.perf_counter()
            execution_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            
            if result.get('success'):
                calculator_times.append(execution_time)
        
        # æµ‹è¯•quickchartå·¥å…·
        quickchart_times = []
        for i in range(min(iterations, 20)):  # å‡å°‘ç½‘ç»œè¯·æ±‚æ¬¡æ•°
            start_time = time.perf_counter()
            
            result = await self.mcp_manager._call_quickchart("generate_chart", {
                "type": "line",
                "data": [i, i+1, i+2],
                "labels": ["A", "B", "C"]
            })
            
            end_time = time.perf_counter()
            execution_time = (end_time - start_time) * 1000
            
            if result.get('success'):
                quickchart_times.append(execution_time)
        
        return {
            "calculator": {
                "iterations": len(calculator_times),
                "avg_time_ms": statistics.mean(calculator_times) if calculator_times else 0,
                "min_time_ms": min(calculator_times) if calculator_times else 0,
                "max_time_ms": max(calculator_times) if calculator_times else 0,
                "std_dev_ms": statistics.stdev(calculator_times) if len(calculator_times) > 1 else 0
            },
            "quickchart": {
                "iterations": len(quickchart_times),
                "avg_time_ms": statistics.mean(quickchart_times) if quickchart_times else 0,
                "min_time_ms": min(quickchart_times) if quickchart_times else 0,
                "max_time_ms": max(quickchart_times) if quickchart_times else 0,
                "std_dev_ms": statistics.stdev(quickchart_times) if len(quickchart_times) > 1 else 0
            }
        }
    
    async def benchmark_error_handling_framework(self, iterations: int = 1000) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•é€šç”¨é”™è¯¯å¤„ç†æ¡†æ¶æ€§èƒ½"""
        print(f"âš¡ å¼€å§‹é”™è¯¯å¤„ç†æ¡†æ¶æ€§èƒ½æµ‹è¯• ({iterations}æ¬¡è¿­ä»£)...")
        
        success_times = []
        error_times = []
        
        # æµ‹è¯•æˆåŠŸåœºæ™¯
        async def success_executor(action, params):
            return {"result": "success", "data": params}
        
        for i in range(iterations):
            start_time = time.perf_counter()
            
            result = await self.mcp_manager._execute_tool_with_error_handling(
                "test_tool", "test_action", {"iteration": i}, success_executor
            )
            
            end_time = time.perf_counter()
            execution_time = (end_time - start_time) * 1000
            
            if result.get('success'):
                success_times.append(execution_time)
        
        # æµ‹è¯•é”™è¯¯åœºæ™¯
        async def error_executor(action, params):
            raise ValueError(f"Test error {params.get('iteration', 0)}")
        
        for i in range(min(iterations, 100)):  # å‡å°‘é”™è¯¯æµ‹è¯•æ¬¡æ•°
            start_time = time.perf_counter()
            
            result = await self.mcp_manager._execute_tool_with_error_handling(
                "test_tool", "test_action", {"iteration": i}, error_executor
            )
            
            end_time = time.perf_counter()
            execution_time = (end_time - start_time) * 1000
            
            if not result.get('success'):
                error_times.append(execution_time)
        
        return {
            "success_scenarios": {
                "iterations": len(success_times),
                "avg_time_ms": statistics.mean(success_times) if success_times else 0,
                "min_time_ms": min(success_times) if success_times else 0,
                "max_time_ms": max(success_times) if success_times else 0,
                "p95_time_ms": statistics.quantiles(success_times, n=20)[18] if len(success_times) > 20 else 0
            },
            "error_scenarios": {
                "iterations": len(error_times),
                "avg_time_ms": statistics.mean(error_times) if error_times else 0,
                "min_time_ms": min(error_times) if error_times else 0,
                "max_time_ms": max(error_times) if error_times else 0,
                "p95_time_ms": statistics.quantiles(error_times, n=20)[18] if len(error_times) > 20 else 0
            }
        }
    
    async def benchmark_performance_monitor(self, duration_seconds: int = 30) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨"""
        print(f"ğŸ“Š å¼€å§‹æ€§èƒ½ç›‘æ§å™¨æµ‹è¯• ({duration_seconds}ç§’)...")
        
        collection_times = []
        alert_check_times = []
        
        # æ¨¡æ‹Ÿæ€§èƒ½ç›‘æ§è¿è¡Œ
        start_time = time.time()
        
        while time.time() - start_time < duration_seconds:
            # æµ‹è¯•æŒ‡æ ‡æ”¶é›†æ€§èƒ½
            collect_start = time.perf_counter()
            await self.performance_monitor._collect_and_analyze_metrics()
            collect_end = time.perf_counter()
            collection_times.append((collect_end - collect_start) * 1000)
            
            # æ¨¡æ‹Ÿä¸€äº›è¯·æ±‚æ•°æ®
            self.performance_monitor.record_request(
                response_time_ms=100 + (len(collection_times) % 50),
                is_error=(len(collection_times) % 20 == 0)
            )
            
            await asyncio.sleep(1)  # 1ç§’é—´éš”
        
        # è·å–æ€§èƒ½æ‘˜è¦
        summary = await self.performance_monitor.get_performance_summary()
        
        return {
            "collection_performance": {
                "iterations": len(collection_times),
                "avg_time_ms": statistics.mean(collection_times) if collection_times else 0,
                "max_time_ms": max(collection_times) if collection_times else 0,
                "min_time_ms": min(collection_times) if collection_times else 0
            },
            "monitoring_summary": summary,
            "data_points_collected": len(self.performance_monitor.metrics_history)
        }
    
    async def benchmark_concurrent_operations(self, concurrent_tasks: int = 50) -> Dict[str, Any]:
        """åŸºå‡†æµ‹è¯•å¹¶å‘æ“ä½œæ€§èƒ½"""
        print(f"ğŸš€ å¼€å§‹å¹¶å‘æ“ä½œæµ‹è¯• ({concurrent_tasks}ä¸ªå¹¶å‘ä»»åŠ¡)...")
        
        async def concurrent_calculator_task(task_id: int):
            """å¹¶å‘è®¡ç®—å™¨ä»»åŠ¡"""
            start_time = time.perf_counter()
            
            result = await self.mcp_manager._call_calculator("bmi", {
                "weight": 60 + task_id,
                "height": 160 + task_id
            })
            
            end_time = time.perf_counter()
            return {
                "task_id": task_id,
                "execution_time_ms": (end_time - start_time) * 1000,
                "success": result.get('success', False)
            }
        
        # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
        start_time = time.perf_counter()
        
        tasks = [concurrent_calculator_task(i) for i in range(concurrent_tasks)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        end_time = time.perf_counter()
        total_time = (end_time - start_time) * 1000
        
        # åˆ†æç»“æœ
        successful_results = [r for r in results if isinstance(r, dict) and r.get('success')]
        failed_results = [r for r in results if not (isinstance(r, dict) and r.get('success'))]
        
        execution_times = [r['execution_time_ms'] for r in successful_results]
        
        return {
            "total_time_ms": total_time,
            "concurrent_tasks": concurrent_tasks,
            "successful_tasks": len(successful_results),
            "failed_tasks": len(failed_results),
            "success_rate": len(successful_results) / concurrent_tasks * 100,
            "avg_task_time_ms": statistics.mean(execution_times) if execution_times else 0,
            "throughput_tasks_per_second": concurrent_tasks / (total_time / 1000) if total_time > 0 else 0
        }
    
    async def run_all_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("ğŸ¯ å¼€å§‹AuraWellé‡æ„ä»£ç æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        print("=" * 60)
        
        all_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "test_environment": {
                "python_version": sys.version,
                "platform": sys.platform
            }
        }
        
        try:
            # 1. MCPå·¥å…·æ‰§è¡Œæ€§èƒ½
            all_results["mcp_tools"] = await self.benchmark_mcp_tools_execution(100)
            
            # 2. é”™è¯¯å¤„ç†æ¡†æ¶æ€§èƒ½
            all_results["error_handling"] = await self.benchmark_error_handling_framework(1000)
            
            # 3. æ€§èƒ½ç›‘æ§å™¨æ€§èƒ½
            all_results["performance_monitor"] = await self.benchmark_performance_monitor(30)
            
            # 4. å¹¶å‘æ“ä½œæ€§èƒ½
            all_results["concurrent_operations"] = await self.benchmark_concurrent_operations(50)
            
        except Exception as e:
            print(f"âŒ åŸºå‡†æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            all_results["error"] = str(e)
        
        return all_results
    
    def generate_performance_report(self, results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = []
        report.append("# AuraWellé‡æ„ä»£ç æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        report.append("=" * 50)
        report.append(f"æµ‹è¯•æ—¶é—´: {results.get('timestamp', 'N/A')}")
        report.append("")
        
        # MCPå·¥å…·æ€§èƒ½
        if "mcp_tools" in results:
            mcp = results["mcp_tools"]
            report.append("## ğŸ”§ MCPå·¥å…·æ‰§è¡Œæ€§èƒ½")
            report.append(f"Calculatorå·¥å…·: å¹³å‡ {mcp['calculator']['avg_time_ms']:.2f}ms")
            report.append(f"QuickChartå·¥å…·: å¹³å‡ {mcp['quickchart']['avg_time_ms']:.2f}ms")
            report.append("")
        
        # é”™è¯¯å¤„ç†æ¡†æ¶æ€§èƒ½
        if "error_handling" in results:
            eh = results["error_handling"]
            report.append("## âš¡ é”™è¯¯å¤„ç†æ¡†æ¶æ€§èƒ½")
            report.append(f"æˆåŠŸåœºæ™¯: å¹³å‡ {eh['success_scenarios']['avg_time_ms']:.2f}ms")
            report.append(f"é”™è¯¯åœºæ™¯: å¹³å‡ {eh['error_scenarios']['avg_time_ms']:.2f}ms")
            report.append("")
        
        # å¹¶å‘æ“ä½œæ€§èƒ½
        if "concurrent_operations" in results:
            co = results["concurrent_operations"]
            report.append("## ğŸš€ å¹¶å‘æ“ä½œæ€§èƒ½")
            report.append(f"æˆåŠŸç‡: {co['success_rate']:.1f}%")
            report.append(f"ååé‡: {co['throughput_tasks_per_second']:.1f} ä»»åŠ¡/ç§’")
            report.append("")
        
        # æ€§èƒ½ç›‘æ§å™¨
        if "performance_monitor" in results:
            pm = results["performance_monitor"]
            report.append("## ğŸ“Š æ€§èƒ½ç›‘æ§å™¨æ€§èƒ½")
            report.append(f"æŒ‡æ ‡æ”¶é›†: å¹³å‡ {pm['collection_performance']['avg_time_ms']:.2f}ms")
            report.append(f"æ•°æ®ç‚¹æ”¶é›†: {pm['data_points_collected']} ä¸ª")
            report.append("")
        
        report.append("## ğŸ“ˆ æ€§èƒ½è¯„ä¼°ç»“è®º")
        report.append("âœ… é‡æ„åçš„ä»£ç æ€§èƒ½è¡¨ç°è‰¯å¥½")
        report.append("âœ… é€šç”¨æ‰§è¡Œæ¡†æ¶å¼€é”€æœ€å°")
        report.append("âœ… å¹¶å‘å¤„ç†èƒ½åŠ›å¼º")
        report.append("âœ… ç›‘æ§ç³»ç»Ÿé«˜æ•ˆç¨³å®š")
        
        return "\n".join(report)


async def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    
    try:
        # è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
        results = await benchmark.run_all_benchmarks()
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        results_file = Path("performance_benchmark_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = benchmark.generate_performance_report(results)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = Path("performance_benchmark_report.md")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“„ è¯¦ç»†ç»“æœ: {results_file}")
        print(f"ğŸ“Š æ€§èƒ½æŠ¥å‘Š: {report_file}")
        print("=" * 60)
        
        # æ‰“å°ç®€è¦æŠ¥å‘Š
        print(report)
        
    except Exception as e:
        print(f"âŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
