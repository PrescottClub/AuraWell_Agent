#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MCPå·¥å…·ç³»ç»Ÿæµ‹è¯•å¥—ä»¶
éªŒè¯MCPå·¥å…·çš„å®Œæ•´åŠŸèƒ½ï¼ŒåŒ…æ‹¬çœŸå®å·¥å…·å’Œå ä½ç¬¦å·¥å…·
"""

import asyncio
import pytest
import logging
import sys
import os
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.aurawell.langchain_agent.mcp_tools_enhanced import EnhancedMCPTools, ToolExecutionMode
from src.aurawell.langchain_agent.mcp_tools_manager_v2 import MCPToolsManagerV2, ToolMode
from src.aurawell.langchain_agent.mcp_performance_monitor import MCPPerformanceMonitor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MCPToolsTestSuite:
    """MCPå·¥å…·æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.enhanced_tools = None
        self.manager_v2 = None
        self.performance_monitor = None
        self.test_results = {}
        
    async def setup(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸš€ è®¾ç½®MCPå·¥å…·æµ‹è¯•ç¯å¢ƒ...")
        
        # åˆå§‹åŒ–å¢å¼ºå·¥å…·ï¼ˆå ä½ç¬¦æ¨¡å¼ï¼Œé¿å…ä¾èµ–é—®é¢˜ï¼‰
        self.enhanced_tools = EnhancedMCPTools(ToolExecutionMode.PLACEHOLDER)
        
        # åˆå§‹åŒ–ç®¡ç†å™¨v2
        self.manager_v2 = MCPToolsManagerV2(ToolMode.PLACEHOLDER)
        await self.manager_v2.initialize()
        
        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        self.performance_monitor = MCPPerformanceMonitor(":memory:")  # ä½¿ç”¨å†…å­˜æ•°æ®åº“
        
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
    
    async def test_enhanced_tools(self) -> Dict[str, Any]:
        """æµ‹è¯•å¢å¼ºå·¥å…·åŠŸèƒ½"""
        logger.info("ğŸ§ª æµ‹è¯•å¢å¼ºMCPå·¥å…·...")
        
        test_tools = [
            'calculator', 'database-sqlite', 'time', 'filesystem', 'brave-search',
            'quickchart', 'fetch', 'sequential-thinking', 'memory', 'weather',
            'run-python', 'github', 'figma'
        ]
        
        results = {}
        
        for tool_name in test_tools:
            try:
                # æµ‹è¯•åŸºæœ¬å·¥å…·è°ƒç”¨
                result = await self.enhanced_tools.call_tool(
                    tool_name=tool_name,
                    action="test",
                    parameters={"test_param": "test_value"}
                )
                
                results[tool_name] = {
                    "success": result.success,
                    "execution_time": result.execution_time,
                    "mode_used": result.mode_used.value,
                    "error": result.error
                }
                
                if result.success:
                    logger.info(f"âœ… {tool_name}: æµ‹è¯•é€šè¿‡ ({result.execution_time:.3f}s)")
                else:
                    logger.warning(f"âš ï¸ {tool_name}: æµ‹è¯•å¤±è´¥ - {result.error}")
                    
            except Exception as e:
                logger.error(f"âŒ {tool_name}: æµ‹è¯•å¼‚å¸¸ - {e}")
                results[tool_name] = {
                    "success": False,
                    "execution_time": 0,
                    "mode_used": "error",
                    "error": str(e)
                }
        
        return results
    
    async def test_specific_tools(self) -> Dict[str, Any]:
        """æµ‹è¯•ç‰¹å®šå·¥å…·çš„è¯¦ç»†åŠŸèƒ½"""
        logger.info("ğŸ” æµ‹è¯•ç‰¹å®šå·¥å…·è¯¦ç»†åŠŸèƒ½...")
        
        results = {}
        
        # æµ‹è¯•è®¡ç®—å™¨
        try:
            calc_result = await self.enhanced_tools.call_tool(
                "calculator", "calculate", {"expression": "2 + 3 * 4"}
            )
            results["calculator_math"] = {
                "success": calc_result.success,
                "result": calc_result.data.get("result"),
                "expected": 14
            }
        except Exception as e:
            results["calculator_math"] = {"success": False, "error": str(e)}
        
        # æµ‹è¯•æ•°æ®åº“
        try:
            db_result = await self.enhanced_tools.call_tool(
                "database-sqlite", "query_health_data", {"user_id": "test_user"}
            )
            results["database_query"] = {
                "success": db_result.success,
                "has_data": "health_metrics" in str(db_result.data)
            }
        except Exception as e:
            results["database_query"] = {"success": False, "error": str(e)}
        
        # æµ‹è¯•æ—¶é—´å·¥å…·
        try:
            time_result = await self.enhanced_tools.call_tool(
                "time", "get_current_time", {}
            )
            results["time_service"] = {
                "success": time_result.success,
                "has_timestamp": "current_time" in str(time_result.data)
            }
        except Exception as e:
            results["time_service"] = {"success": False, "error": str(e)}
        
        return results
    
    async def test_manager_v2(self) -> Dict[str, Any]:
        """æµ‹è¯•ç®¡ç†å™¨v2åŠŸèƒ½"""
        logger.info("ğŸ¯ æµ‹è¯•MCPå·¥å…·ç®¡ç†å™¨v2...")
        
        results = {}
        
        try:
            # æµ‹è¯•æ™ºèƒ½å·¥å…·è°ƒç”¨
            call_result = await self.manager_v2.call_tool_smart(
                "calculator", "calculate", {"expression": "10 + 5"}
            )
            
            results["smart_call"] = {
                "success": call_result.success,
                "mode_used": call_result.mode_used.value,
                "execution_time": call_result.execution_time
            }
            
            # æµ‹è¯•æ€§èƒ½æŠ¥å‘Š
            performance_report = self.manager_v2.get_performance_report()
            results["performance_report"] = {
                "has_report": bool(performance_report),
                "tool_mode": performance_report.get("tool_mode"),
                "has_recommendations": "recommendations" in performance_report
            }
            
        except Exception as e:
            logger.error(f"âŒ ç®¡ç†å™¨v2æµ‹è¯•å¤±è´¥: {e}")
            results["error"] = str(e)
        
        return results
    
    async def test_performance_monitor(self) -> Dict[str, Any]:
        """æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨"""
        logger.info("ğŸ“Š æµ‹è¯•æ€§èƒ½ç›‘æ§å™¨...")
        
        results = {}
        
        try:
            # è®°å½•ä¸€äº›æµ‹è¯•æŒ‡æ ‡
            self.performance_monitor.record_metric(
                tool_name="test_tool",
                action="test_action",
                execution_time=0.1,
                success=True,
                mode_used="placeholder"
            )
            
            # åˆ·æ–°æŒ‡æ ‡åˆ°æ•°æ®åº“
            await self.performance_monitor._flush_metrics()
            
            # è·å–æ€§èƒ½æ‘˜è¦
            summary = await self.performance_monitor.get_performance_summary(hours=1)
            results["performance_summary"] = {
                "has_summary": bool(summary),
                "total_calls": summary.get("overall", {}).get("total_calls", 0)
            }
            
            # æ£€æŸ¥å‘Šè­¦
            alerts = await self.performance_monitor.check_alerts()
            results["alerts_check"] = {
                "alerts_count": len(alerts),
                "has_alerts": len(alerts) > 0
            }
            
        except Exception as e:
            logger.error(f"âŒ æ€§èƒ½ç›‘æ§å™¨æµ‹è¯•å¤±è´¥: {e}")
            results["error"] = str(e)
        
        return results
    
    async def test_error_handling(self) -> Dict[str, Any]:
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        logger.info("ğŸ›¡ï¸ æµ‹è¯•é”™è¯¯å¤„ç†...")
        
        results = {}
        
        try:
            # æµ‹è¯•æ— æ•ˆå·¥å…·åç§°
            invalid_result = await self.enhanced_tools.call_tool(
                "invalid_tool", "test", {}
            )
            results["invalid_tool"] = {
                "handled_gracefully": not invalid_result.success,
                "has_error_message": bool(invalid_result.error)
            }
            
            # æµ‹è¯•æ— æ•ˆå‚æ•°
            invalid_param_result = await self.enhanced_tools.call_tool(
                "calculator", "calculate", {"invalid_param": "test"}
            )
            results["invalid_params"] = {
                "handled_gracefully": True,  # å ä½ç¬¦å·¥å…·åº”è¯¥èƒ½å¤„ç†ä»»ä½•å‚æ•°
                "success": invalid_param_result.success
            }
            
        except Exception as e:
            logger.error(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            results["error"] = str(e)
        
        return results
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸ¯ å¼€å§‹è¿è¡ŒMCPå·¥å…·å®Œæ•´æµ‹è¯•å¥—ä»¶...")
        
        await self.setup()
        
        test_results = {
            "test_timestamp": asyncio.get_event_loop().time(),
            "enhanced_tools": await self.test_enhanced_tools(),
            "specific_tools": await self.test_specific_tools(),
            "manager_v2": await self.test_manager_v2(),
            "performance_monitor": await self.test_performance_monitor(),
            "error_handling": await self.test_error_handling()
        }
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_tests = 0
        passed_tests = 0
        
        for category, results in test_results.items():
            if category == "test_timestamp":
                continue
            if isinstance(results, dict):
                for test_name, result in results.items():
                    if isinstance(result, dict) and "success" in result:
                        total_tests += 1
                        if result["success"]:
                            passed_tests += 1
        
        test_results["summary"] = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": total_tests - passed_tests,
            "success_rate": (passed_tests / max(total_tests, 1)) * 100
        }
        
        logger.info(f"ğŸ‰ æµ‹è¯•å®Œæˆ: {passed_tests}/{total_tests} é€šè¿‡ ({test_results['summary']['success_rate']:.1f}%)")
        
        return test_results
    
    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        logger.info("ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
        
        if self.performance_monitor:
            await self.performance_monitor.cleanup()
        
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    test_suite = MCPToolsTestSuite()
    
    try:
        results = await test_suite.run_all_tests()
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print("\n" + "="*60)
        print("MCPå·¥å…·æµ‹è¯•ç»“æœæŠ¥å‘Š")
        print("="*60)
        
        summary = results.get("summary", {})
        print(f"æ€»æµ‹è¯•æ•°: {summary.get('total_tests', 0)}")
        print(f"é€šè¿‡æµ‹è¯•: {summary.get('passed_tests', 0)}")
        print(f"å¤±è´¥æµ‹è¯•: {summary.get('failed_tests', 0)}")
        print(f"æˆåŠŸç‡: {summary.get('success_rate', 0):.1f}%")
        
        # å¦‚æœæˆåŠŸç‡ä½äº80%ï¼Œè¿”å›é”™è¯¯ç 
        if summary.get('success_rate', 0) < 80:
            print("\nâš ï¸ è­¦å‘Š: æµ‹è¯•æˆåŠŸç‡ä½äº80%ï¼Œè¯·æ£€æŸ¥MCPå·¥å…·é…ç½®")
            return 1
        else:
            print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ŒMCPå·¥å…·ç³»ç»Ÿè¿è¡Œæ­£å¸¸")
            return 0
            
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        return 1
    finally:
        await test_suite.cleanup()


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
