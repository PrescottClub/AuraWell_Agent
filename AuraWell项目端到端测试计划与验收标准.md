# AuraWell项目端到端测试计划与验收标准

**文档版本**: v1.0  
**编制日期**: 2025年7月12日  
**编制人**: 首席质量保证工程师  
**审核状态**: 待审核  

---

## 第一部分：核心测试场景 (Key Test Scenarios)

### 场景一：完整的用户反馈与优化闭环

**测试目标**: 验证从用户接收AI健康建议到反馈收集，再到PromptOptimizerService分析的完整数据流

**前置条件**:
- 用户已登录系统
- 数据库中存在用户档案数据
- PromptPerformanceService正常运行
- 前端ChatMessage组件包含反馈按钮

**测试步骤**:
1. 用户发送健康咨询消息："我最近总是失眠，有什么改善建议吗？"
2. AI系统调用PromptManager生成个性化健康建议
3. 前端展示AI回复，包含👍👎反馈按钮
4. 用户点击👎按钮（负面反馈）
5. 前端调用`/api/v1/chat/feedback`接口提交反馈
6. 后端PromptPerformanceService记录反馈数据到数据库
7. PromptOptimizerService定期分析收集到的反馈数据

**预期成功结果**:
- ✅ AI回复内容相关且有用
- ✅ 反馈按钮正常显示和响应
- ✅ 反馈数据成功存储到`prompt_performance_logs`表
- ✅ 反馈记录包含完整字段：user_rating, response_relevance, feedback_updated_at
- ✅ PromptOptimizerService能够基于负面反馈生成优化建议
- ✅ 系统日志记录完整的反馈流程

**验收标准**:
- 反馈提交成功率 ≥ 99%
- 数据库记录完整性 = 100%
- 反馈处理响应时间 ≤ 500ms

---

### 场景二：MCP工具的真实调用

**测试目标**: 验证用户问题触发MCP工具调用的完整流程，特别是QuickChart图表生成

**前置条件**:
- MCP工具管理器正常初始化
- QuickChart API可访问
- 用户具有有效的健康数据

**测试步骤**:
1. 用户提问："请帮我生成一个我最近一周的体重变化图表"
2. IntentAnalyzer分析用户意图，识别需要使用quickchart工具
3. MCPToolsManager调用`_call_quickchart`方法
4. 系统构建图表配置（类型：line，数据：体重数据，标签：日期）
5. 调用QuickChart API生成图表URL
6. 将图表URL返回给用户

**预期成功结果**:
- ✅ 意图分析正确识别图表生成需求
- ✅ MCP工具成功调用QuickChart API
- ✅ 返回有效的图表URL
- ✅ 图表数据准确反映用户的体重变化
- ✅ 图表样式符合AuraWell设计规范
- ✅ 整个流程在5秒内完成

**验收标准**:
- 工具调用成功率 ≥ 95%
- 图表生成准确性 = 100%
- 端到端响应时间 ≤ 5秒

---

### 场景三：监控仪表板的数据有效性

**测试目标**: 验证系统管理员能够通过监控仪表板获取真实的系统健康指标

**前置条件**:
- 监控仪表板服务运行在端口8001
- PerformanceMonitor后台服务正常运行
- 数据库连接正常
- 系统有一定的运行数据

**测试步骤**:
1. 系统管理员访问 `http://localhost:8001`
2. 仪表板页面加载系统概览数据
3. 验证数据库连接状态显示
4. 验证AI模型状态检查
5. 查看用户统计信息（总用户数、活跃用户数）
6. 访问 `/api/metrics` 获取详细指标
7. 访问 `/api/health` 进行健康检查

**预期成功结果**:
- ✅ 仪表板页面在3秒内完全加载
- ✅ 数据库状态显示为"healthy"
- ✅ AI模型状态显示为"healthy"
- ✅ 用户统计数据准确（与数据库实际数据一致）
- ✅ 性能指标实时更新（CPU、内存、磁盘使用率）
- ✅ API端点返回有效的JSON数据
- ✅ 页面自动刷新功能正常工作

**验收标准**:
- 页面加载时间 ≤ 3秒
- 数据准确性 = 100%
- API响应时间 ≤ 1秒
- 监控数据更新频率 = 30秒

---

## 第二部分：自动化测试脚本生成 (Automated Test Script Generation)

### 已生成的测试脚本

#### 1. `tests/test_mcp_tools_manager.py`

**覆盖范围**:
- ✅ `_call_calculator`方法的单元测试（BMI、BMR、TDEE计算）
- ✅ `_call_quickchart`方法的参数化测试（line、bar、pie、doughnut图表）
- ✅ 数据库工具调用测试
- ✅ 错误处理和异常情况测试
- ✅ 集成测试（意图分析 + 工具执行）
- ✅ 并发调用性能测试

**关键测试用例**:
```python
@pytest.mark.parametrize("chart_type,data,labels", [
    ("line", [10, 20, 30, 25, 35], ["周一", "周二", "周三", "周四", "周五"]),
    ("bar", [65, 70, 68, 72, 69], ["1月", "2月", "3月", "4月", "5月"]),
    ("pie", [30, 25, 20, 25], ["蛋白质", "碳水", "脂肪", "其他"])
])
```

#### 2. `tests/test_performance_monitor.py`

**覆盖范围**:
- ✅ 性能指标收集验证
- ✅ 告警系统测试（CPU、内存、磁盘、响应时间、错误率）
- ✅ 请求记录和统计功能
- ✅ 监控生命周期管理
- ✅ 数据获取和摘要生成
- ✅ 边界条件和错误处理

**关键测试用例**:
```python
@pytest.mark.asyncio
async def test_check_alerts_cpu_high(self, performance_monitor, mock_psutil):
    mock_psutil.cpu_percent.return_value = 85.0  # 超过75%阈值
    await performance_monitor._collect_metrics()
    await performance_monitor._check_alerts()
    assert len(performance_monitor.active_alerts) > 0
```

---

## 第三部分：手动测试用例 (Manual Test Cases)

| 测试项 | 测试步骤 | 预期结果 |
|--------|----------|----------|
| **API文档生成** | 1. 运行 `python scripts/generate_api_docs.py`<br>2. 检查 `docs/api/` 目录<br>3. 打开 `swagger.html` 文件<br>4. 验证API端点列表 | - 生成4种格式文档（JSON、Swagger、ReDoc、Postman）<br>- Swagger UI正常显示所有API端点<br>- 文档内容准确反映实际API结构 |
| **API文档访问** | 1. 启动FastAPI服务<br>2. 访问 `/docs` 端点<br>3. 测试API交互功能<br>4. 验证请求/响应示例 | - Swagger UI界面完整加载<br>- 可以展开API端点查看详情<br>- Try it out功能正常工作<br>- 响应示例格式正确 |
| **Prompt Playground单版本测试** | 1. 访问 `/admin/prompt-playground`<br>2. 选择"单版本测试"标签<br>3. 配置prompt版本为v3_1<br>4. 输入测试问题<br>5. 点击"运行测试" | - 页面正常加载，无JavaScript错误<br>- 版本选择器显示可用版本<br>- 测试结果在5秒内返回<br>- 显示响应时间和token使用量 |
| **Prompt Playground版本对比** | 1. 切换到"版本对比"标签<br>2. 选择版本A和版本B<br>3. 输入相同测试问题<br>4. 执行并排对比测试<br>5. 查看差异分析 | - 两个版本同时执行测试<br>- 并排显示响应结果<br>- 高亮显示关键差异<br>- 性能指标对比准确 |
| **用户反馈UI功能** | 1. 发送健康咨询消息<br>2. 等待AI回复<br>3. 点击👍按钮<br>4. 点击👎按钮<br>5. 检查反馈状态 | - 反馈按钮正常显示<br>- 点击后按钮状态改变<br>- 反馈提交成功提示<br>- 后台正确记录反馈数据 |
| **监控仪表板实时性** | 1. 访问监控仪表板<br>2. 观察指标更新<br>3. 手动刷新页面<br>4. 检查API端点响应<br>5. 验证告警功能 | - 指标每30秒自动更新<br>- 手动刷新立即更新数据<br>- 所有API端点返回有效数据<br>- 异常情况触发告警显示 |
| **数据库连接状态** | 1. 停止数据库服务<br>2. 访问监控仪表板<br>3. 查看数据库状态<br>4. 重启数据库服务<br>5. 验证状态恢复 | - 数据库断开时显示"error"状态<br>- 错误信息描述准确<br>- 数据库恢复后状态变为"healthy"<br>- 状态变化在1分钟内反映 |
| **AI模型健康检查** | 1. 模拟AI服务异常<br>2. 触发健康检查<br>3. 查看模型状态<br>4. 恢复AI服务<br>5. 验证状态更新 | - AI异常时显示"error"状态<br>- 包含具体错误信息<br>- 服务恢复后状态正常<br>- 健康检查响应时间<10秒 |
| **性能指标准确性** | 1. 运行高负载测试<br>2. 观察CPU/内存指标<br>3. 对比系统监控工具<br>4. 验证告警触发<br>5. 检查历史数据 | - 指标数据与系统实际状态一致<br>- 高负载时正确触发告警<br>- 历史数据保留完整<br>- 指标计算准确无误 |

---

## 验收标准总结

### 功能完整性标准
- ✅ 所有核心用户旅程100%可执行
- ✅ API端点覆盖率≥95%
- ✅ 前端功能组件覆盖率≥90%

### 性能标准
- ✅ API响应时间≤2秒（95%请求）
- ✅ 页面加载时间≤3秒
- ✅ 系统可用性≥99.5%

### 质量标准
- ✅ 自动化测试覆盖率≥80%
- ✅ 关键路径测试通过率=100%
- ✅ 错误处理覆盖率≥90%

### 安全标准
- ✅ 用户认证机制正常
- ✅ 数据传输加密
- ✅ 敏感信息保护

---

**测试执行建议**:
1. 优先执行核心测试场景，确保主要功能正常
2. 运行自动化测试套件，验证代码质量
3. 执行手动测试用例，确保用户体验
4. 进行性能和负载测试，验证系统稳定性
5. 最后进行安全测试和渗透测试

**风险评估**:
- 🔴 高风险：用户反馈数据丢失
- 🟡 中风险：MCP工具调用超时
- 🟢 低风险：监控仪表板显示延迟

此测试计划将确保AuraWell项目达到生产环境部署标准。
