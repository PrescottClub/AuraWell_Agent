"""
å¢å¼ºæ£€ç´¢æ–¹æ³•ï¼Œé€šè¿‡å‘é‡ç›¸ä¼¼åº¦æŸ¥æ‰¾ï¼Œæ£€ç´¢æ•°æ®åº“ä¸­å‰Kä¸ªå’Œç”¨æˆ·æŸ¥è¯¢ç›¸å…³çš„å­—æ®µï¼Œä¹Ÿæä¾›å°†æ–‡æ¡£è§£æå¹¶ä¸Šä¼ è‡³å‘é‡æ•°æ®åº“çš„æ–¹æ³•
è¯¥æ‹“å±•ä½¿ç”¨é˜¿é‡Œå·´å·´æä¾›çš„äº‘æœåŠ¡ï¼Œä½¿ç”¨å‰è¯·åŠ¡å¿…é€šè¿‡pipå‘½ä»¤å®‰è£…å¥½å¯¹åº”çš„åº“
ç›®å‰é€šè¿‡.envè§£æå¯†é’¥ï¼Œæœªæ¥å°†é€šè¿‡KMSå¯†é’¥ç®¡ç†ç³»ç»Ÿè§£æå¯†é’¥
rag_utilsæ˜¯ä¸€ä¸ªè¾…åŠ©RAGExtensionæ ¸å¿ƒé€»è¾‘è¿è¡Œçš„å·¥å…·ç±»ï¼Œå…¶æœ¬èº«ä¸ä¼šè°ƒç”¨API
"""
from rag_utils import get_file_type, process_list, get_download_path
from alibabacloud_docmind_api20220711.client import Client as docmind_api20220711Client
from alibabacloud_tea_openapi import models as open_api_models
from alibabacloud_docmind_api20220711 import models as docmind_api20220711_models
from alibabacloud_tea_util import models as util_models
from alibabacloud_credentials.client import Client as CredClient
from alibabacloud_credentials.models import Config as CredentialsConfig
from dashvector import Doc

from openai import OpenAI
from dotenv import load_dotenv
import uuid
import os
import time
import dashvector
import numpy as np
import platform

def normalize_file_path(file_path: str) -> str:
    """
    è·¨å¹³å°æ–‡ä»¶è·¯å¾„æ ‡å‡†åŒ–å¤„ç†

    Args:
        file_path (str): è¾“å…¥çš„æ–‡ä»¶è·¯å¾„

    Returns:
        str: æ ‡å‡†åŒ–åçš„ç»å¯¹è·¯å¾„
    """
    # å°†è·¯å¾„è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    abs_path = os.path.abspath(file_path)

    # æ ‡å‡†åŒ–è·¯å¾„åˆ†éš”ç¬¦
    normalized_path = os.path.normpath(abs_path)

    return normalized_path

def load_api_keys():
    """
    ä».envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­åŠ è½½APIå¯†é’¥
    ä¼˜å…ˆå°è¯•ä».envæ–‡ä»¶åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½

    Returns:
        dict: åŒ…å«æ‰€æœ‰APIå¯†é’¥çš„å­—å…¸ï¼Œå¦‚æœæŸä¸ªå¯†é’¥æœªæ‰¾åˆ°åˆ™å€¼ä¸ºNone
        bool: æ˜¯å¦æˆåŠŸåŠ è½½äº†æ‰€æœ‰å¿…éœ€çš„å¯†é’¥
    """
    keys = {
        "ALIBABA_CLOUD_ACCESS_KEY_ID": None,
        "ALIBABA_CLOUD_ACCESS_KEY_SECRET": None,
        "DASHSCOPE_API_KEY": None,
        "DASH_VECTOR_API": None
    }

    # æ–¹æ³•1: å°è¯•ä».envæ–‡ä»¶åŠ è½½
    try:
        # è·å–å½“å‰æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
        current_file_path = os.path.abspath(__file__)
        # è·å–å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆå³ aurawell/ragï¼‰
        current_dir = os.path.dirname(current_file_path)
        # è·å–é¡¹ç›®æ ¹ç›®å½•
        project_root = os.path.dirname(os.path.dirname(current_dir))
        # æ„å»º .env æ–‡ä»¶çš„å®Œæ•´è·¯å¾„
        dotenv_path = os.path.join(project_root, '.env')

        # å¦‚æœ.envæ–‡ä»¶å­˜åœ¨ï¼Œå°è¯•åŠ è½½
        if os.path.exists(dotenv_path):
            load_dotenv(dotenv_path=dotenv_path)
            print(f"âœ… å°è¯•ä».envæ–‡ä»¶åŠ è½½å¯†é’¥: {dotenv_path}")
        else:
            print(f"âš ï¸  .envæ–‡ä»¶ä¸å­˜åœ¨: {dotenv_path}")
    except Exception as e:
        print(f"âš ï¸  ä».envæ–‡ä»¶åŠ è½½å¯†é’¥å¤±è´¥: {e}")

    # æ–¹æ³•2: ä»ç¯å¢ƒå˜é‡è¯»å–ï¼ˆåŒ…æ‹¬.envåŠ è½½åçš„ç¯å¢ƒå˜é‡ï¼‰
    for key in keys.keys():
        value = os.getenv(key)
        if value:
            keys[key] = value
            print(f"âœ… æˆåŠŸåŠ è½½å¯†é’¥: {key}")
        else:
            print(f"âŒ æœªæ‰¾åˆ°å¯†é’¥: {key}")

    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¿…éœ€çš„å¯†é’¥éƒ½å·²åŠ è½½
    missing_keys = [key for key, value in keys.items() if not value]
    success = len(missing_keys) == 0

    if success:
        print("ğŸ‰ æ‰€æœ‰APIå¯†é’¥åŠ è½½æˆåŠŸ")
    else:
        print(f"âš ï¸  ç¼ºå°‘ä»¥ä¸‹å¯†é’¥: {missing_keys}")

    return keys, success

class Document:
    def __init__(self):
        """
        ä».envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­åŠ è½½APIå¯†é’¥ï¼Œè¿™ä¸ªç±»éœ€è¦è¯»å–çš„api keyæœ‰ï¼š
        ALIBABA_CLOUD_ACCESS_KEY_ID=SECRET
        ALIBABA_CLOUD_ACCESS_KEY_SECRET=SECRET
        DASHSCOPE_API_KEY=SECRET
        DASH_VECTOR_API=SECRET

        ä¼˜å…ˆä».envæ–‡ä»¶åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½
        """
        print("ğŸ”„ Documentç±»åˆå§‹åŒ–ï¼šå¼€å§‹åŠ è½½APIå¯†é’¥...")

        # åŠ è½½APIå¯†é’¥
        keys, success = load_api_keys()

        if not success:
            raise ValueError("âŒ æ— æ³•åŠ è½½å¿…éœ€çš„APIå¯†é’¥ï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è®¾ç½®")

        # è®¾ç½®å®ä¾‹å˜é‡
        self.access_key_id = keys["ALIBABA_CLOUD_ACCESS_KEY_ID"]
        self.access_key_secret = keys["ALIBABA_CLOUD_ACCESS_KEY_SECRET"]
        self.dash_scope_key = keys["DASHSCOPE_API_KEY"]
        self.dash_vector_key = keys["DASH_VECTOR_API"]

        # è®¿é—®çš„åŸŸå
        # é˜¿é‡Œäº‘docmindæœåŠ¡çš„èŠ‚ç‚¹åœ°å€ï¼Œå†™æˆç±»å±æ€§æ˜¯ä¸ºäº†æ–¹ä¾¿æœªæ¥æ›´æ¢
        self.docmind_endpoint = f'docmind-api.cn-hangzhou.aliyuncs.com'
        # é˜¿é‡Œäº‘çš„å‘é‡æ•°æ®åº“æœåŠ¡çš„èŠ‚ç‚¹åœ°å€
        self.vectorDB_endpoint = "vrs-cn-6sa4axaiv0001c.dashvector.cn-shanghai.aliyuncs.com"
        self.bailian_endpoint = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        self.region_id = "cn-hangzhou"

        print("âœ… Documentç±»åˆå§‹åŒ–å®Œæˆ")
    def __doc_analysation(self, file_path:str):
        """
        ä½¿ç”¨é˜¿é‡Œäº‘çš„docmindæœåŠ¡å¯¹æ–‡æ¡£è¿›è¡Œè§£æï¼Œè¿”å›è§£æç»“æœ
        :param file_path: æ–‡æ¡£è·¯å¾„
        :return: è§£æç»“æœ
        """
        # æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿è·¨å¹³å°å…¼å®¹æ€§
        normalized_path = normalize_file_path(file_path)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(normalized_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {normalized_path}")

        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ
        file_extension = get_file_type(normalized_path)
        supported_formats = ["pdf", "docx", "xlsx"]
        if file_extension not in supported_formats:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_extension}. ä»…æ”¯æŒ: {supported_formats}")
        # æ„é€  Credentialï¼Œç›´æ¥ä»æœ¬åœ°è¯»å–ï¼Œåœ¨æ­£å¼éƒ¨ç½²æ—¶éœ€è¦æ›´æ”¹
        cred_config = CredentialsConfig(
            type="access_key",
            access_key_id=self.access_key_id,
            access_key_secret=self.access_key_secret
        )
        # åŠ è½½å®Œæ¯•ï¼Œç°åœ¨å¯ä»¥åˆå§‹åŒ–ç±»
        cred = CredClient(config=cred_config)
        config = open_api_models.Config(
            # é€šè¿‡credentialsè·å–é…ç½®ä¸­çš„AccessKey ID
            access_key_id=cred.get_credential().get_access_key_id(),
            # é€šè¿‡credentialsè·å–é…ç½®ä¸­çš„AccessKey Secret
            access_key_secret=cred.get_credential().get_access_key_secret()
        )
        config.read_timeout = 2400 # 10é¡µä»¥ä¸Šçš„æ–‡ä»¶å¯èƒ½çœŸçš„è¦ä¸Šä¼ 30åˆ†é’Ÿä»¥ä¸Šï¼Œå®åœ¨æ˜¯å¾ˆéš¾ç»·
        config.connect_timeout = 1200 # äººåœ¨ç€æ–¯ï¼Œè¿æ¥ä¸ç¨³å®šï¼Œæ‰€ä»¥åªèƒ½å…ˆè¿™ä¹ˆè®¾ç½®
        config.endpoint = self.docmind_endpoint
        client = docmind_api20220711Client(config)
        request = docmind_api20220711_models.SubmitDocParserJobAdvanceRequest(
            # file_url_object : æœ¬åœ°æ–‡ä»¶æµ
            file_url_object=open(normalized_path, "rb"),
            # æ–‡æ¡£ç±»å‹æ˜¯å¿…é¡»ä¸Šä¼ çš„å‚æ•°
            file_name_extension=get_file_type(normalized_path)
        )
        runtime = util_models.RuntimeOptions(
            read_timeout=24000,
            connect_timeout=12000
        )
        response = client.submit_doc_parser_job_advance(request, runtime)
        print(response.body) # æŸ¥çœ‹åˆ°åº•æ˜¯ä»€ä¹ˆè¿”å›

        # æ£€æŸ¥å“åº”æ˜¯å¦æˆåŠŸ
        if response.body.data is None:
            print(f"Error: {response.body}")
            raise Exception(f"Document parsing failed: {response.body}")

        job_id = response.body.data.id
        request = docmind_api20220711_models.QueryDocParserStatusRequest(
            # id :  ä»»åŠ¡æäº¤æ¥å£è¿”å›çš„id
            id=job_id
        )
        response = client.query_doc_parser_status(request)
        service_status = response.body.data.status
        wait_time = 0.0 # ç›®å‰æ²¡æœ‰æƒ³åˆ°ä¸€ä¸ªåˆé€‚çš„æ£€æµ‹è¶…æ—¶çš„åŠæ³•ï¼Œå…ˆå°±è¿™ä¹ˆå°†å°±ä¸€ä¸‹å§
        while service_status != "success" and wait_time < 2400:
            time.sleep(0.5)
            wait_time += 0.5
            response = client.query_doc_parser_status(request)
            service_status = response.body.data.status
            print(service_status)
        request = docmind_api20220711_models.GetDocParserResultRequest(
            # id :  ä»»åŠ¡æäº¤æ¥å£è¿”å›çš„id
            id=job_id,
            layout_step_size=100,
            layout_num=0
        )
        # å¤åˆ¶ä»£ç è¿è¡Œè¯·è‡ªè¡Œæ‰“å° API çš„è¿”å›å€¼
        response = client.get_doc_parser_result(request)
        # APIè¿”å›å€¼æ ¼å¼å±‚çº§ä¸º body -> data -> å…·ä½“å±æ€§ã€‚å¯æ ¹æ®ä¸šåŠ¡éœ€è¦æ‰“å°ç›¸åº”çš„ç»“æœã€‚è·å–å±æ€§å€¼å‡ä»¥å°å†™å¼€å¤´
        # è·å–è¿”å›ç»“æœã€‚å»ºè®®å…ˆæŠŠresponse.body.dataè½¬æˆjsonï¼Œç„¶åå†ä»jsoné‡Œé¢å–å…·ä½“éœ€è¦çš„å€¼ã€‚
        print(response.body)
        return response.body.data
    def file_Parsing(self, file_path:str)->str:
        """
        ä»é˜¿é‡Œäº‘ DocMind API è¿”å›çš„ data å­—æ®µä¸­æå–æ‰€æœ‰ markdownContentï¼Œ
        æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„ Markdown å­—ç¬¦ä¸²ï¼Œå¹¶ï¼š
        - å»é™¤ç©ºç™½è¡Œ
        - åˆå¹¶è¿ç»­çš„æ–‡æœ¬å—
        """
        raw_text = self.__doc_analysation(file_path)
        full_markdown = ""
        layouts = raw_text.get("layouts",  []) if raw_text.get("layouts",  None) is not None else []
        for layout in layouts:
            markdown = layout.get("markdownContent", None) if layout.get("markdownContent", None) is not None else ""
            full_markdown += markdown
        # æŒ‰è¡Œåˆ†å‰²
        lines = full_markdown.splitlines()
        cleaned_lines = []
        buffer = ""
        for line in lines:
            stripped_line = line.strip()
            if stripped_line == "":  # ç©ºè¡Œ
                if buffer:
                    cleaned_lines.append(buffer)
                    buffer = ""
            else:
                buffer += " " + stripped_line if buffer else stripped_line
        # æ·»åŠ æœ€åå¯èƒ½å­˜åœ¨çš„æ®µè½
        if buffer:
            cleaned_lines.append(buffer)
        return "\n".join(cleaned_lines)

    def __is_reference_content(self, text: str) -> bool:
        """
        åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºæ–‡çŒ®å¼•ç”¨å†…å®¹

        Args:
            text (str): å¾…æ£€æµ‹çš„æ–‡æœ¬

        Returns:
            bool: True å¦‚æœæ˜¯å¼•ç”¨å†…å®¹ï¼ŒFalse å¦åˆ™
        """
        if not text or not isinstance(text, str):
            return False

        text = text.strip()

        # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œä¸å¤ªå¯èƒ½æ˜¯å¼•ç”¨
        if len(text) < 10:
            return False

        # æ£€æµ‹å¼•ç”¨çš„ç‰¹å¾æ¨¡å¼
        import re

        # æ¨¡å¼1: ä»¥æ–¹æ‹¬å·æ•°å­—å¼€å¤´çš„å¼•ç”¨ [1], [19], [1-3] ç­‰
        bracket_number_pattern = r'^\[\d+(-\d+)?\]'
        if re.match(bracket_number_pattern, text):
            return True

        # æ¨¡å¼2: åŒ…å«DOIçš„å¼•ç”¨
        doi_pattern = r'DOI\s*:\s*10\.\d+/'
        if re.search(doi_pattern, text, re.IGNORECASE):
            return True

        # æ¨¡å¼3: åŒ…å«æœŸåˆŠå¼•ç”¨æ ¼å¼çš„æ–‡æœ¬
        # æ£€æµ‹æ ¼å¼ï¼šä½œè€…å,æ ‡é¢˜[J].æœŸåˆŠå,å¹´ä»½,å·(æœŸ):é¡µç 
        journal_pattern = r'[^,]+,.*?\[J\]\..*?,\d{4},.*?:'
        if re.search(journal_pattern, text):
            return True

        # æ¨¡å¼4: åŒ…å«å¤šä¸ªä½œè€…çš„å¼•ç”¨æ ¼å¼ (å§“å,å§“å,ç­‰.)
        multiple_authors_pattern = r'[^,]+,[^,]+,ç­‰\.'
        if re.search(multiple_authors_pattern, text):
            return True

        # æ¨¡å¼5: åŒ…å«è‹±æ–‡æœŸåˆŠå¼•ç”¨æ ¼å¼
        # æ£€æµ‹æ ¼å¼ï¼šAuthor A, Author B, et al. Title[J]. Journal, Year, Volume(Issue): Pages.
        english_journal_pattern = r'[A-Z][a-z]+\s+[A-Z]{1,3},.*?et\s+al\..*?\[J\]\..*?,\s*\d{4}'
        if re.search(english_journal_pattern, text):
            return True

        # æ¨¡å¼6: åŒ…å«ISBNæˆ–ISSNçš„å¼•ç”¨
        isbn_issn_pattern = r'(ISBN|ISSN)\s*[:ï¼š]\s*[\d-]+'
        if re.search(isbn_issn_pattern, text, re.IGNORECASE):
            return True

        # æ¨¡å¼7: æ£€æµ‹å¼•ç”¨ä¸­å¸¸è§çš„æ ‡ç‚¹ç¬¦å·å¯†åº¦
        # å¼•ç”¨é€šå¸¸åŒ…å«å¤§é‡çš„é€—å·ã€å¥å·ã€å†’å·ç­‰
        punctuation_count = sum(1 for char in text if char in ',.;:()[]{}')
        if len(text) > 50 and punctuation_count / len(text) > 0.15:
            # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦åŒ…å«å¹´ä»½
            year_pattern = r'\b(19|20)\d{2}\b'
            if re.search(year_pattern, text):
                return True

        return False

    def __content_vectorised(self, raw_content):
        # raw_content å¯¹åº”çš„æ˜¯æ–‡æ¡£è§£ææ–¹æ³•è¿”å›çš„ response.body.data
        discard_type = () # è¿”å›å€¼ä¸­ï¼Œå¿½ç•¥å…¶ä¸­typeå¯¹åº”çš„å€¼ä¸­ä¸ºè¿™äº›å€¼çš„å†…å®¹ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ä¸å¯¹åº”ä»»ä½•å…·ä½“å†…å®¹
        discard_subtype = ("doc_title", "title", "para_title")  # è¿”å›å€¼ä¸­ï¼Œå¿½ç•¥å…¶ä¸­sub_titleå¯¹åº”çš„å€¼ä¸­ä¸ºè¿™äº›å€¼çš„å†…å®¹ï¼Œå› ä¸ºå®ƒä»¬é€šå¸¸ä¸å¯¹åº”ä»»ä½•å…·ä½“å†…å®¹
        # raw_content æŒ‡çš„æ˜¯PDFæ–‡æ¡£è§£ææœåŠ¡è¿”å›å€¼ä¸­çš„dataå­—æ®µï¼Œå…¶ä½™çš„è¾“å…¥éƒ½ä¼šå¯¼è‡´é”™è¯¯
        query_list = []
        layouts = raw_content.get("layouts", []) if raw_content.get("layouts", None) is not None else []
        for layout in layouts:
            if layout.get("subType", "Unknown") in discard_subtype:
                continue
            elif layout.get("type", "Unknown") == "table": # å¯¹è¿”å›å†…å®¹ä¸­è¢«æ ‡è®°ä¸ºtableçš„å…ƒç´ è¿›è¡Œç‰¹åˆ«å¤„ç†ï¼Œä»¥å»é™¤åœ¨è¯†åˆ«è¿‡ç¨‹ä¸­äº§ç”Ÿçš„éé¢„æœŸå­—ç¬¦
                table_content = layout.get("markdownContent", "")
                table_content = table_content.split("\n")
                table_content = process_list(table_content)
                for s in table_content:
                    if type(s) == type("hello"):
                        # æ£€æŸ¥æ˜¯å¦ä¸ºå¼•ç”¨å†…å®¹ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
                        if not self.__is_reference_content(s):
                            query_list.append(s) # å°†è¡¨æ ¼ä¸­çš„æ¯ä¸€ä¸ªæ ¼å­ä¸­çš„å†…å®¹åŠ å…¥è‡³åˆ—è¡¨ä¸­
                    else:
                        continue

            markdown = layout.get("markdownContent", None) if layout.get("markdownContent", None) is not None else ""
            # æ£€æŸ¥markdownå†…å®¹æ˜¯å¦ä¸ºå¼•ç”¨ï¼Œå¦‚æœä¸æ˜¯å¼•ç”¨æ‰æ·»åŠ åˆ°åˆ—è¡¨ä¸­
            if markdown and not self.__is_reference_content(markdown):
                query_list.append(markdown)
        print(f"The length of query_list is: {len(query_list)}")
        client = OpenAI(
            api_key=self.dash_scope_key,  # ç›´æ¥é…ç½®api keyä»¥è®¿é—®é˜¿é‡Œäº‘æä¾›çš„æœåŠ¡
            base_url=self.bailian_endpoint  # ç™¾ç‚¼æœåŠ¡çš„base_url
        )

        # é˜¿é‡Œäº‘çš„è¯åµŒå…¥æ¥å£ä¸€æ¬¡æ€§æœ€å¤šæ¥å—10ä¸ªå­—ç¬¦ä¸²è¾“å…¥ï¼Œæ‰€ä»¥è¦åˆ†å¼€æ¥ä¸Šä¼ 
        vector_list = []
        for i in range(0,len(query_list)-10,10):
            completion = client.embeddings.create(
                model="text-embedding-v4",
                input=query_list[i:i+10], # ä¸€æ¬¡æ€§æ‰¹ä¸Šä¼ 10ä¸ªæ–‡æœ¬å—è‡³é˜¿é‡Œäº‘ç™¾ç‚¼API
                dimensions=1024,  # æŒ‡å®šå‘é‡ç»´åº¦ï¼ˆä»… text-embedding-v3åŠ text-embedding-v4æ”¯æŒè¯¥å‚æ•°ï¼‰
                # ç›®å‰é¡¹ç›®å¯ç”¨çš„DashVectorä»…æ”¯æŒå›ºå®šé•¿åº¦çš„å‘é‡ï¼Œæ ¹æ®ç®€å•è°ƒæŸ¥ï¼Œé€‰æ‹©1024ç»´è¿™ä¸ªæ¯”è¾ƒé€šç”¨çš„é€‰æ‹©
                encoding_format="float"
            )
            vector_list.append(completion.data)
        flatten_vector_list = [
            np.array(item.embedding,dtype=float)
            for sublist in vector_list
            for item in sublist
        ]
        rawcontent_vector_pair = list(zip(query_list, flatten_vector_list))
        return rawcontent_vector_pair
        # print(completion.model_dump_json())
        # return vector_list, query_list # åŒæ—¶è¿”å›åŸæ–‡å†…å®¹å’Œå…¶å¯¹åº”çš„å‘é‡
    def file2VectorDB(self, file_path:str)->bool:
        response = self.__doc_analysation(file_path)
        vector_pairs = self.__content_vectorised(response)
        client = dashvector.Client(
            api_key=self.dash_vector_key,
            endpoint=self.vectorDB_endpoint
        )
        collection = client.get(name='simple_collection')
        for i in range(len(vector_pairs)):
            ret = collection.insert(
                Doc(
                    # æµ‹è¯•æ—¶ï¼Œéœ€è¦ä½¿ç”¨iæŒ‡å®šå‘é‡idï¼Œä½¿ç”¨è¦†ç›–æ“ä½œé˜²æ­¢æ•°æ®åº“å­˜å‚¨å› æµ‹è¯•è€Œå¢å¤§ï¼Œå¯¼è‡´ä¸å¿…è¦çš„èŠ±é”€
                    id=str(uuid.uuid4()),
                    vector=vector_pairs[i][1],
                    fields={
                        "raw_text":  vector_pairs[i][0],
                        "sub_title": "test"
                    }
                )
            )
            assert ret # åˆ¤æ–­æ’å…¥æ“ä½œæ˜¯å¦æˆåŠŸ
        return True

class UserRetrieve:
    def __init__(self):
        """
        ä».envæ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­åŠ è½½APIå¯†é’¥ï¼Œè¿™ä¸ªç±»éœ€è¦è¯»å–çš„api keyæœ‰ï¼š
        DASHSCOPE_API_KEY=SECRET
        DASH_VECTOR_API=SECRET

        ä¼˜å…ˆä».envæ–‡ä»¶åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½
        """
        print("ğŸ”„ UserRetrieveç±»åˆå§‹åŒ–ï¼šå¼€å§‹åŠ è½½APIå¯†é’¥...")

        # åŠ è½½APIå¯†é’¥
        keys, success = load_api_keys()

        # UserRetrieveåªéœ€è¦éƒ¨åˆ†å¯†é’¥
        required_keys = ["DASHSCOPE_API_KEY", "DASH_VECTOR_API"]
        missing_keys = [key for key in required_keys if not keys.get(key)]

        if missing_keys:
            raise ValueError(f"âŒ UserRetrieveç±»ç¼ºå°‘å¿…éœ€çš„APIå¯†é’¥: {missing_keys}")

        # è®¾ç½®å®ä¾‹å˜é‡
        self.dash_scope_key = keys["DASHSCOPE_API_KEY"]
        self.dash_vector_key = keys["DASH_VECTOR_API"]

        # è®¿é—®çš„åŸŸå
        # é˜¿é‡Œäº‘çš„å‘é‡æ•°æ®åº“æœåŠ¡çš„èŠ‚ç‚¹åœ°å€
        self.vectorDB_endpoint = "vrs-cn-6sa4axaiv0001c.dashvector.cn-shanghai.aliyuncs.com"
        self.bailian_endpoint = "https://dashscope.aliyuncs.com/compatible-mode/v1"

        print("âœ… UserRetrieveç±»åˆå§‹åŒ–å®Œæˆ")
    def __user_query_vectorised(self, raw_user_query:str):
        # ç›®å‰é˜¿é‡Œäº‘å·²ç»æ¨å‡ºDashScopeè°ƒç”¨ï¼Œä½†æ˜¯OpenAIå…¼å®¹æ–¹æ³•å¯¹åº”çš„æ–‡æ¡£æ›´æ¸…æ™°ï¼Œå…ˆç”¨ç€
        client = OpenAI(
            api_key=self.dash_scope_key,  # ç›´æ¥é…ç½®api keyä»¥è®¿é—®é˜¿é‡Œäº‘æä¾›çš„æœåŠ¡
            base_url=self.bailian_endpoint  # ç™¾ç‚¼æœåŠ¡çš„base_url
        )
        completion = client.embeddings.create(
            model="text-embedding-v4",
            input=raw_user_query,  # ä¸€æ¬¡æ€§æ‰¹ä¸Šä¼ 10ä¸ªæ–‡æœ¬å—è‡³é˜¿é‡Œäº‘ç™¾ç‚¼API
            dimensions=1024,  # æŒ‡å®šå‘é‡ç»´åº¦ï¼ˆä»… text-embedding-v3åŠ text-embedding-v4æ”¯æŒè¯¥å‚æ•°ï¼‰
            encoding_format="float"
        )
        print(type(completion.data)) # è¯·æ³¨æ„ï¼Œè¿”å›å€¼æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º1çš„åˆ—è¡¨ï¼Œå› æ­¤éœ€è¦å–åˆ—è¡¨çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œç„¶åå†è½¬åŒ–
        embeded_result = (raw_user_query , np.array(completion.data[0].embedding, dtype=float))
        return embeded_result
    def retrieve_topK(self, user_query:str, k:int):
        relate_words = []
        client = dashvector.Client(
            api_key=self.dash_vector_key,
            endpoint=self.vectorDB_endpoint
        )
        collection = client.get(name='simple_collection')
        vectorised_user_input = self.__user_query_vectorised(user_query)


        ret = collection.query(
            vector=vectorised_user_input[1],
            topk=k,
            output_fields=['raw_text', 'sub_title'],
            include_vector=True
        )
        if ret:
            print("query success")
        retrieve_result = []
        for content in ret.output:
            retrieve_result.append(content.fields["raw_text"])
        return  retrieve_result

if __name__ == "__main__":
    # test_user = UserRetrieve()
    # retrieve_list = test_user.retrieve_topK("æ¯æ—¥è¥å…»å»ºè®®", 3)
    # print(len(retrieve_list))
    # print(retrieve_list)

    # ä½¿ç”¨è·¨å¹³å°è·¯å¾„å¤„ç†
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sample_doc_path = os.path.join(current_dir, "testMaterials", "ä¸­å›½æˆå¹´äººè‚‰ç±»é£Ÿç‰©æ‘„å…¥ä¸ä»£è°¢ç»¼åˆå¾çš„ç›¸å…³æ€§ç ”ç©¶.pdf")

    obj = Document()
    result = obj.file2VectorDB(sample_doc_path)
    print(result)

