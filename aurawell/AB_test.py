#!/usr/bin/env python3
"""
AuraWell ABæµ‹è¯•æ–‡ä»¶
ç”¨äºå¿«é€Ÿæµ‹è¯•ä¸åŒæ¨¡å‹çš„è°ƒç”¨æ•ˆæœ

ä½¿ç”¨æ–¹æ³•:
python aurawell/AB_test.py --model deepseek-v3
python aurawell/AB_test.py --model qwen-turbo
python aurawell/AB_test.py --compare
"""

import os
import sys
import argparse
import asyncio
import time
from typing import Dict, List, Optional
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from src.aurawell.core.deepseek_client import DeepSeekClient
from src.aurawell.services.model_fallback_service import get_model_fallback_service


class ABTestRunner:
    """ABæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.test_queries = [
            "è¯·ç®€å•ä»‹ç»ä¸€ä¸‹å¥åº·é¥®é£Ÿçš„åŸºæœ¬åŸåˆ™",
            "å¦‚ä½•åˆ¶å®šä¸€ä¸ªé€‚åˆä¸Šç­æ—çš„è¿åŠ¨è®¡åˆ’ï¼Ÿ",
            "é«˜è¡€å‹æ‚£è€…åœ¨é¥®é£Ÿä¸Šéœ€è¦æ³¨æ„ä»€ä¹ˆï¼Ÿ",
            "è¯·è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯BMIï¼Œå¦‚ä½•è®¡ç®—ï¼Ÿ",
            "å¤±çœ çš„å¸¸è§åŸå› æœ‰å“ªäº›ï¼Ÿå¦‚ä½•æ”¹å–„ç¡çœ è´¨é‡ï¼Ÿ"
        ]
        
    async def test_single_model(self, model_name: str) -> Dict:
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
        print("=" * 50)
        
        results = {
            "model": model_name,
            "tests": [],
            "total_time": 0,
            "success_count": 0,
            "error_count": 0
        }
        
        try:
            # åˆ›å»ºå®¢æˆ·ç«¯
            client = DeepSeekClient()
            
            for i, query in enumerate(self.test_queries, 1):
                print(f"\nğŸ“ æµ‹è¯• {i}/{len(self.test_queries)}: {query[:50]}...")
                
                start_time = time.time()
                test_result = {
                    "query": query,
                    "success": False,
                    "response": "",
                    "response_time": 0,
                    "error": None
                }
                
                try:
                    # æ„å»ºæ¶ˆæ¯
                    messages = [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¥åº·é¡¾é—®ï¼Œè¯·æä¾›å‡†ç¡®ã€å®ç”¨çš„å¥åº·å»ºè®®ã€‚"},
                        {"role": "user", "content": query}
                    ]
                    
                    # è°ƒç”¨æ¨¡å‹
                    response = client.get_deepseek_response(
                        messages=messages,
                        model_name=model_name,
                        temperature=0.7,
                        max_tokens=512
                    )
                    
                    test_result["success"] = True
                    test_result["response"] = response.content[:200] + "..." if len(response.content) > 200 else response.content
                    test_result["response_time"] = time.time() - start_time
                    results["success_count"] += 1
                    
                    print(f"âœ… æˆåŠŸ ({test_result['response_time']:.2f}s)")
                    print(f"ğŸ“„ å“åº”: {test_result['response'][:100]}...")
                    
                except Exception as e:
                    test_result["error"] = str(e)
                    test_result["response_time"] = time.time() - start_time
                    results["error_count"] += 1
                    
                    print(f"âŒ å¤±è´¥ ({test_result['response_time']:.2f}s): {str(e)[:100]}")
                
                results["tests"].append(test_result)
                results["total_time"] += test_result["response_time"]
                
                # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                await asyncio.sleep(1)
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            results["init_error"] = str(e)
        
        return results
    
    async def compare_models(self) -> Dict:
        """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹"""
        print("\nğŸ”„ å¼€å§‹ABå¯¹æ¯”æµ‹è¯•")
        print("=" * 60)
        
        models_to_test = [
            os.getenv("DEEPSEEK_SERIES_V3", "deepseek-v3"),
            os.getenv("QWEN_FAST", "qwen-turbo")
        ]
        
        comparison_results = {
            "models": models_to_test,
            "results": {},
            "summary": {}
        }
        
        # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
        for model in models_to_test:
            results = await self.test_single_model(model)
            comparison_results["results"][model] = results
        
        # ç”Ÿæˆå¯¹æ¯”æ€»ç»“
        comparison_results["summary"] = self._generate_comparison_summary(comparison_results["results"])
        
        return comparison_results
    
    def _generate_comparison_summary(self, results: Dict) -> Dict:
        """ç”Ÿæˆå¯¹æ¯”æ€»ç»“"""
        summary = {
            "best_performance": None,
            "best_reliability": None,
            "recommendations": []
        }
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        avg_times = {}
        success_rates = {}
        
        for model, result in results.items():
            if result["success_count"] > 0:
                avg_times[model] = result["total_time"] / len(result["tests"])
                success_rates[model] = result["success_count"] / len(result["tests"])
            else:
                avg_times[model] = float('inf')
                success_rates[model] = 0
        
        # æ‰¾å‡ºæœ€ä½³æ€§èƒ½æ¨¡å‹ï¼ˆå“åº”æ—¶é—´æœ€çŸ­ï¼‰
        if avg_times:
            summary["best_performance"] = min(avg_times.keys(), key=lambda k: avg_times[k])
        
        # æ‰¾å‡ºæœ€ä½³å¯é æ€§æ¨¡å‹ï¼ˆæˆåŠŸç‡æœ€é«˜ï¼‰
        if success_rates:
            summary["best_reliability"] = max(success_rates.keys(), key=lambda k: success_rates[k])
        
        # ç”Ÿæˆå»ºè®®
        for model, result in results.items():
            if result["success_count"] == len(result["tests"]):
                summary["recommendations"].append(f"âœ… {model}: 100%æˆåŠŸç‡ï¼Œå¹³å‡å“åº”æ—¶é—´{avg_times[model]:.2f}s")
            elif result["success_count"] > 0:
                summary["recommendations"].append(f"âš ï¸ {model}: {success_rates[model]*100:.1f}%æˆåŠŸç‡ï¼Œå¹³å‡å“åº”æ—¶é—´{avg_times[model]:.2f}s")
            else:
                summary["recommendations"].append(f"âŒ {model}: å®Œå…¨å¤±è´¥")
        
        return summary
    
    def print_results(self, results: Dict):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        if "results" in results:  # å¯¹æ¯”æµ‹è¯•ç»“æœ
            print("\nğŸ“Š ABæµ‹è¯•å¯¹æ¯”ç»“æœ")
            print("=" * 60)
            
            for model, result in results["results"].items():
                print(f"\nğŸ¤– æ¨¡å‹: {model}")
                print(f"   æˆåŠŸ: {result['success_count']}/{len(result['tests'])}")
                print(f"   æ€»è€—æ—¶: {result['total_time']:.2f}s")
                if result['success_count'] > 0:
                    print(f"   å¹³å‡å“åº”æ—¶é—´: {result['total_time']/len(result['tests']):.2f}s")
            
            print(f"\nğŸ† æµ‹è¯•æ€»ç»“:")
            summary = results["summary"]
            if summary["best_performance"]:
                print(f"   æœ€ä½³æ€§èƒ½: {summary['best_performance']}")
            if summary["best_reliability"]:
                print(f"   æœ€ä½³å¯é æ€§: {summary['best_reliability']}")
            
            print(f"\nğŸ’¡ å»ºè®®:")
            for rec in summary["recommendations"]:
                print(f"   {rec}")
                
        else:  # å•æ¨¡å‹æµ‹è¯•ç»“æœ
            print(f"\nğŸ“Š æ¨¡å‹ {results['model']} æµ‹è¯•ç»“æœ")
            print("=" * 50)
            print(f"æˆåŠŸ: {results['success_count']}/{len(results['tests'])}")
            print(f"å¤±è´¥: {results['error_count']}/{len(results['tests'])}")
            print(f"æ€»è€—æ—¶: {results['total_time']:.2f}s")
            if results['success_count'] > 0:
                print(f"å¹³å‡å“åº”æ—¶é—´: {results['total_time']/len(results['tests']):.2f}s")


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="AuraWell ABæµ‹è¯•å·¥å…·")
    parser.add_argument("--model", type=str, help="æŒ‡å®šè¦æµ‹è¯•çš„æ¨¡å‹åç§°")
    parser.add_argument("--compare", action="store_true", help="å¯¹æ¯”æµ‹è¯•å¤šä¸ªæ¨¡å‹")
    parser.add_argument("--output", type=str, help="ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶")
    
    args = parser.parse_args()
    
    runner = ABTestRunner()
    
    if args.compare:
        results = await runner.compare_models()
    elif args.model:
        results = await runner.test_single_model(args.model)
    else:
        # é»˜è®¤æµ‹è¯•å½“å‰é…ç½®çš„æ¨¡å‹
        default_model = os.getenv("DEEPSEEK_SERIES_V3", "deepseek-v3")
        print(f"ğŸ¯ ä½¿ç”¨é»˜è®¤æ¨¡å‹: {default_model}")
        results = await runner.test_single_model(default_model)
    
    # æ‰“å°ç»“æœ
    runner.print_results(results)
    
    # ä¿å­˜ç»“æœ
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")


if __name__ == "__main__":
    asyncio.run(main())
